{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概述\n",
    "\n",
    "与带有Attention机制的Seq2Seq所不同的是，Transformer模型是一个纯基于自注意力机制（self-attention mechanism）的架构，不包含任何循环神经网络（Recurrent Neural Network， RNN）和卷积神经网络（Convolutional Neural Network，CNN）。整个模型由\n",
    "## 数据准备\n",
    "\n",
    "我们本次使用的数据集为**Multi30K数据集**，它是一个大规模的图像-文本数据集，包含30K+图片，每张图片对应两类不同的文本描述：\n",
    "- 英语描述，及对应的德语翻译；\n",
    "- 五个独立的、非翻译而来的英语和德语描述，描述中包含的细节并不相同；\n",
    "\n",
    "因其收集的不同语言对于图片的描述相互独立，所以训练出的模型可以更好地适用于有噪声的多模态内容。\n",
    "\n",
    "![avatar](./assets/Multi30K.png)\n",
    "> 图片来源：Elliott, D., Frank, S., Sima’an, K., & Specia, L. (2016). Multi30K: Multilingual English-German Image Descriptions. CoRR, 1605.00459.\n",
    "\n",
    "首先，我们需要下载如下依赖：\n",
    "\n",
    "- 分词工具：`pip install spacy`\n",
    "- 德语/英语分词器：`python -m spacy download de_core_news_sm`，`python -m spacy download en_core_web_sm`\n",
    "\n",
    "### 数据下载模块\n",
    "\n",
    "使用`download`进行数据下载，并将`tar.gz`文件解压到指定文件夹。\n",
    "\n",
    "下载好的数据集目录结构如下：\n",
    "\n",
    "```text\n",
    "home_path/.mindspore_examples\n",
    "├─test\n",
    "│      test2016.de\n",
    "│      test2016.en\n",
    "│      test2016.fr\n",
    "│\n",
    "├─train\n",
    "│      train.de\n",
    "│      train.en\n",
    "│\n",
    "└─valid\n",
    "        val.de\n",
    "        val.en\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replace is False and data exists, so doing nothing. Use replace=True to re-download the data.\n",
      "Replace is False and data exists, so doing nothing. Use replace=True to re-download the data.\n",
      "Replace is False and data exists, so doing nothing. Use replace=True to re-download the data.\n"
     ]
    }
   ],
   "source": [
    "from download import download\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# 训练、验证、测试数据集下载地址\n",
    "urls = {\n",
    "    'train': 'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/training.tar.gz',\n",
    "    'valid': 'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz',\n",
    "    'test': 'http://www.quest.dcs.shef.ac.uk/wmt17_files_mmt/mmt_task1_test2016.tar.gz'\n",
    "}\n",
    "\n",
    "# 指定保存路径为 `home_path/.mindspore_examples`\n",
    "cache_dir = Path.home() / '.mindspore_examples'\n",
    "\n",
    "train_path = download(urls['train'], os.path.join(cache_dir, 'train'), kind='tar.gz')\n",
    "valid_path = download(urls['valid'], os.path.join(cache_dir, 'valid'), kind='tar.gz')\n",
    "test_path = download(urls['test'], os.path.join(cache_dir, 'test'), kind='tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理\n",
    "\n",
    "在使用数据进行模型训练等操作时，我们需要对数据进行预处理，流程如下：\n",
    "\n",
    "1. 加载数据集，目前数据为句子形式的文本，需要进行分词，即将句子拆解为单独的词元（token，可以为字符或者单词）；\n",
    "    - 分词可以使用`spaCy`创建分词器（tokenizer）：`de_core_news_sm`，`en_core_web_sm`，需要手动下载；\n",
    "    - 分词后，去除多余的空格，统一大小写等；\n",
    "2. 将每个词元映射到从0开始的数字索引中（为节约存储空间，可过滤掉词频低的词元），词元和数字索引所构成的集合叫做词典（vocabulary）；\n",
    "3. 添加特殊占位符，标明序列的起始与结束，统一序列长度，并创建数据迭代器；\n",
    "\n",
    "#### 数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from functools import partial\n",
    "\n",
    "class Multi30K():\n",
    "    \"\"\"Multi30K数据集加载器\n",
    "\n",
    "    加载Multi30K数据集并处理为一个Python迭代对象。\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.data = self._load(path)\n",
    "\n",
    "    def _load(self, path):\n",
    "        def tokenize(text, spacy_lang):\n",
    "            # 去除多余空格，统一大小写\n",
    "            text = text.rstrip()\n",
    "            return [tok.text.lower() for tok in spacy_lang.tokenizer(text)]\n",
    "\n",
    "        # 加载英、德语分词器\n",
    "        tokenize_de = partial(tokenize, spacy_lang=spacy.load('de_core_news_sm'))\n",
    "        tokenize_en = partial(tokenize, spacy_lang=spacy.load('en_core_web_sm'))\n",
    "\n",
    "        # 读取Multi30K数据，并进行分词\n",
    "        members = {i.split('.')[-1]: i for i in os.listdir(path)}\n",
    "        de_path = os.path.join(path, members['de'])\n",
    "        en_path = os.path.join(path, members['en'])\n",
    "        with open(de_path, 'r') as de_file:\n",
    "            de = de_file.readlines()[:-1]\n",
    "            de = [tokenize_de(i) for i in de]\n",
    "        with open(en_path, 'r') as en_file:\n",
    "            en = en_file.readlines()[:-1]\n",
    "            en = [tokenize_en(i) for i in en]\n",
    "\n",
    "        return list(zip(de, en))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, test_dataset = Multi30K(train_path), Multi30K(valid_path), Multi30K(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对解压和分词结果进行测试，打印测试数据集第一组英德语文本，可以看到每一个单词和标点符号已经被单独分离出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de = ['ein', 'mann', 'mit', 'einem', 'orangefarbenen', 'hut', ',', 'der', 'etwas', 'anstarrt', '.']\n",
      "en = ['a', 'man', 'in', 'an', 'orange', 'hat', 'starring', 'at', 'something', '.']\n"
     ]
    }
   ],
   "source": [
    "for de, en in test_dataset:\n",
    "    print(f'de = {de}')\n",
    "    print(f'en = {en}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"通过词频字典，构建词典\"\"\"\n",
    "\n",
    "    special_tokens = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "    def __init__(self, word_count_dict, min_freq=1):\n",
    "        self.word2idx = {}\n",
    "        for idx, tok in enumerate(self.special_tokens):\n",
    "            self.word2idx[tok] = idx\n",
    "\n",
    "        # 过滤低词频的词元\n",
    "        filted_dict = {\n",
    "            w: c\n",
    "            for w, c in word_count_dict.items() if c >= min_freq\n",
    "        }\n",
    "        for w, _ in filted_dict.items():\n",
    "            self.word2idx[w] = len(self.word2idx)\n",
    "\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "\n",
    "        self.bos_idx = self.word2idx['<bos>']  # 特殊占位符：序列开始\n",
    "        self.eos_idx = self.word2idx['<eos>']  # 特殊占位符：序列结束\n",
    "        self.pad_idx = self.word2idx['<pad>']  # 特殊占位符：补充字符\n",
    "        self.unk_idx = self.word2idx['<unk>']  # 特殊占位符：低词频词元或未曾出现的词元\n",
    "\n",
    "    def _word2idx(self, word):\n",
    "        \"\"\"单词映射至数字索引\"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            return self.unk_idx\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def _idx2word(self, idx):\n",
    "        \"\"\"数字索引映射至单词\"\"\"\n",
    "        if idx not in self.idx2word:\n",
    "            raise ValueError('input index is not in vocabulary.')\n",
    "        return self.idx2word[idx]\n",
    "\n",
    "    def encode(self, word_or_list):\n",
    "        \"\"\"将单个单词或单词数组映射至单个数字索引或数字索引数组\"\"\"\n",
    "        if isinstance(word_or_list, list):\n",
    "            return [self._word2idx(i) for i in word_or_list]\n",
    "        return self._word2idx(word_or_list)\n",
    "\n",
    "    def decode(self, idx_or_list):\n",
    "        \"\"\"将单个数字索引或数字索引数组映射至单个单词或单词数组\"\"\"\n",
    "        if isinstance(idx_or_list, list):\n",
    "            return [self._idx2word(i) for i in idx_or_list]\n",
    "        return self._idx2word(idx_or_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过自定义词频字典进行测试，我们可以看到词典已去除词频少于2的词元c，并加入了默认的四个特殊占位符，故词典整体长度为：4 - 1 + 4 = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count = {'a':20, 'b':10, 'c':1, 'd':2}\n",
    "\n",
    "vocab = Vocab(word_count, min_freq=2)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用`collections`中的`Counter`和`OrderedDict`统计英/德语每个单词在整体文本中出现的频率。构建词频字典，然后再将词频字典转为词典。\n",
    "\n",
    "在分配数字索引时有一个小技巧：常用的词元对应数值较小的索引，这样可以节约空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def build_vocab(dataset):\n",
    "    de_words, en_words = [], []\n",
    "    for de, en in dataset:\n",
    "        de_words.extend(de)\n",
    "        en_words.extend(en)\n",
    "\n",
    "    de_count_dict = OrderedDict(sorted(Counter(de_words).items(), key=lambda t: t[1], reverse=True))\n",
    "    en_count_dict = OrderedDict(sorted(Counter(en_words).items(), key=lambda t: t[1], reverse=True))\n",
    "\n",
    "    return Vocab(de_count_dict, min_freq=2), Vocab(en_count_dict, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in de vocabulary: 7853\n"
     ]
    }
   ],
   "source": [
    "de_vocab, en_vocab = build_vocab(train_dataset)\n",
    "print('Unique tokens in de vocabulary:', len(de_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据迭代器\n",
    "\n",
    "数据预处理的最后一步是创建数据迭代器，我们在进一步处理数据（包括批处理，添加起始和终止符号，统一序列长度）后，将数据以张量的形式返回。\n",
    "\n",
    "创建数据迭代器需要如下参数：\n",
    "\n",
    "- `dataset`：分词后的数据集\n",
    "- `de_vocab`：德语词典\n",
    "- `en_vocab`：英语词典\n",
    "- `batch_size`：批量大小，即一个batch中包含多少个序列\n",
    "- `max_len`：序列最大长度，为最长有效文本长度 + 2（序列开始、序列结束占位符），如不满则补齐，如超过则丢弃\n",
    "- `drop_remainder`：是否在最后一个batch未满时，丢弃该batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "\n",
    "class Iterator():\n",
    "    \"\"\"创建数据迭代器\"\"\"\n",
    "    def __init__(self, dataset, de_vocab, en_vocab, batch_size, max_len=32, drop_reminder=False):\n",
    "        self.dataset = dataset\n",
    "        self.de_vocab = de_vocab\n",
    "        self.en_vocab = en_vocab\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.max_len = max_len\n",
    "        self.drop_reminder = drop_reminder\n",
    "\n",
    "        length = len(self.dataset) // batch_size\n",
    "        self.len = length if drop_reminder else length + 1  # 批量数量\n",
    "\n",
    "    def __call__(self):\n",
    "        def pad(idx_list, vocab, max_len):\n",
    "            \"\"\"统一序列长度，并记录有效长度\"\"\"\n",
    "            idx_pad_list, idx_len = [], []\n",
    "            # 当前序列度超过最大长度时，将超出的部分丢弃；当前序列长度小于最大长度时，用占位符补齐\n",
    "            for i in idx_list:\n",
    "                if len(i) > max_len - 2:\n",
    "                    idx_pad_list.append(\n",
    "                        [vocab.bos_idx] + i[:max_len-2] + [vocab.eos_idx]\n",
    "                    )\n",
    "                    idx_len.append(max_len)\n",
    "                else:\n",
    "                    idx_pad_list.append(\n",
    "                        [vocab.bos_idx] + i + [vocab.eos_idx] + [vocab.pad_idx] * (max_len - len(i) - 2)\n",
    "                    )\n",
    "                    idx_len.append(len(i) + 2)\n",
    "            return idx_pad_list, idx_len\n",
    "\n",
    "        def sort_by_length(src, trg):\n",
    "            \"\"\"对德/英语的字段长度进行排序\"\"\"\n",
    "            data = zip(src, trg)\n",
    "            data = sorted(data, key=lambda t: len(t[0]), reverse=True)\n",
    "            return zip(*list(data))\n",
    "\n",
    "        def encode_and_pad(batch_data, max_len):\n",
    "            \"\"\"将批量中的文本数据转换为数字索引，并统一每个序列的长度\"\"\"\n",
    "            # 将当前批量数据中的词元转化为索引\n",
    "            src_data, trg_data = zip(*batch_data)\n",
    "            src_idx = [self.de_vocab.encode(i) for i in src_data]\n",
    "            trg_idx = [self.en_vocab.encode(i) for i in trg_data]\n",
    "\n",
    "            # 统一序列长度\n",
    "            src_idx, trg_idx = sort_by_length(src_idx, trg_idx)\n",
    "            src_idx_pad, src_len = pad(src_idx, de_vocab, max_len)\n",
    "            trg_idx_pad, _ = pad(trg_idx, en_vocab, max_len)\n",
    "\n",
    "            return src_idx_pad, src_len, trg_idx_pad\n",
    "\n",
    "        for i in range(self.len):\n",
    "            # 获取当前批量的数据\n",
    "            if i == self.len - 1 and not self.drop_reminder:\n",
    "                batch_data = self.dataset[i * self.batch_size:]\n",
    "            else:\n",
    "                batch_data = self.dataset[i * self.batch_size: (i+1) * self.batch_size]\n",
    "\n",
    "            src_idx, src_len, trg_idx = encode_and_pad(batch_data, self.max_len)\n",
    "            # 将序列数据转换为tensor\n",
    "            yield mindspore.Tensor(src_idx, mindspore.int32), \\\n",
    "                mindspore.Tensor(src_len, mindspore.int32), \\\n",
    "                mindspore.Tensor(trg_idx, mindspore.int32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = Iterator(train_dataset, de_vocab, en_vocab, batch_size=128, max_len=32, drop_reminder=True)\n",
    "valid_iterator = Iterator(valid_dataset, de_vocab, en_vocab, batch_size=128, max_len=32, drop_reminder=False)\n",
    "test_iterator = Iterator(test_dataset, de_vocab, en_vocab, batch_size=128, max_len=32, drop_reminder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型解析\n",
    "\n",
    "### Transformer 架构\n",
    "\n",
    "与Seq2Seq类似，编码器+解码器+中间信息传递\n",
    "\n",
    "### 位置编码（Positional Encoding）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.ops as ops\n",
    "import mindspore.nn as nn\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Cell):\n",
    "    \"\"\"位置编码\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, dropout=0.1, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=1 - dropout)\n",
    "\n",
    "        # 位置信息\n",
    "        # shape = [1, max len, embed dim]\n",
    "        self.pos = ops.fill(compute_dtype, (1, max_len, embed_dim), 0)\n",
    "        angle = ops.arange(end=max_len, dtype=compute_dtype).reshape(\n",
    "            -1, 1) / ops.pow(\n",
    "                10000,\n",
    "                ops.arange(end=embed_dim, step=2, dtype=compute_dtype) /\n",
    "                embed_dim)\n",
    "        self.pos[:, :, 0::2] = ops.sin(angle)\n",
    "        self.pos[:, :, 1::2] = ops.cos(angle)\n",
    "\n",
    "    def construct(self, x):\n",
    "        # 将位置编码截取至x同等大小\n",
    "        x = x + self.pos[:, :x.shape[1], :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多头注意力（Multi-Head Attention）\n",
    "\n",
    "- 自注意力\n",
    "- 多头注意力\n",
    "- 带掩码的多头注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.ops as ops\n",
    "\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k, pad_idx, broadcast=False):\n",
    "    \"\"\"注意力掩码：识别序列中的<pad>占位符\n",
    "\n",
    "    Args:\n",
    "        seq_q (Tensor): query序列，shape = [batch size, query len]\n",
    "        seq_k (Tensor): key序列，shape = [batch size, key len]\n",
    "        pad_idx (Tensor): key序列<pad>占位符对应的数字索引\n",
    "        broadcast (bool): 是否需要广播机制。默认：False\n",
    "            如True，返回shape = [batch size, query len, key len]；如False，返回shape = [batch size, key len]\n",
    "    \"\"\"\n",
    "    batch_size, len_q = seq_q.shape\n",
    "    batch_size, len_k = seq_k.shape\n",
    "\n",
    "    # 如果序列中元素对应<pad>占位符，则该位置在mask中对应元素为True\n",
    "    # shape = []\n",
    "    pad_attn_mask = ops.equal(seq_k, pad_idx)\n",
    "\n",
    "    if broadcast:\n",
    "        # 增加额外的维度\n",
    "        # shape = [batch size, 1, key len]\n",
    "        pad_attn_mask = pad_attn_mask.expand_dims(1)\n",
    "        # 将掩码广播到[batch size, query len, key len]\n",
    "        pad_attn_mask = ops.broadcast_to(pad_attn_mask,\n",
    "                                         (batch_size, len_q, len_k))\n",
    "\n",
    "    return pad_attn_mask\n",
    "\n",
    "\n",
    "def get_attn_subsequent_mask(seq_q, seq_k):\n",
    "    \"\"\"生成时间掩码，使decoder在第t时刻只能看到序列的前t-1个元素\n",
    "    \n",
    "    Args:\n",
    "        seq_q (Tensor): query序列，shape = [batch size, query len]\n",
    "        seq_k (Tensor): key序列，shape = [batch size, key len]\n",
    "        pad_idx (Tensor): key序列<pad>占位符对应的数字索引\n",
    "    \"\"\"\n",
    "    _, len_q = seq_q.shape\n",
    "    _, len_k = seq_k.shape\n",
    "    # 生成三角矩阵\n",
    "    # shape = [query len, key len]\n",
    "    ones = ops.ones((len_q, len_k), mindspore.float32)\n",
    "    subsequent_mask = ones.triu(diagonal=1)\n",
    "    # 在第0维增加额外维度\n",
    "    # shape = [1, query len, key len]\n",
    "    subsequent_mask = subsequent_mask.expand_dims(0)\n",
    "    return subsequent_mask\n",
    "\n",
    "\n",
    "def get_enc_dec_mask(src, trg, src_pad_idx, trg_pad_idx):\n",
    "    \"\"\"获取encoder与decoder中的掩码\"\"\"\n",
    "    # encoder 自注意力<pad>占位符掩码\n",
    "    # shape = [batch size, src len]\n",
    "    enc_self_attn_mask = get_attn_pad_mask(src, src, src_pad_idx)\n",
    "\n",
    "    # decoder 带有时间限制的自注意力掩码\n",
    "    # 分别计算<pad>占位符掩码与时间掩码，并合并\n",
    "    # shape = [batch size, trg len, trg len]\n",
    "    dec_self_attn_pad_mask = get_attn_pad_mask(trg,\n",
    "                                               trg,\n",
    "                                               trg_pad_idx,\n",
    "                                               broadcast=True)\n",
    "    dec_self_attn_subsequent_pad_mask = get_attn_subsequent_mask(trg, trg)\n",
    "    dec_self_attn_mask = dec_self_attn_pad_mask + dec_self_attn_subsequent_pad_mask\n",
    "    dec_self_attn_mask = ops.gt((dec_self_attn_mask), 0)\n",
    "\n",
    "    # decoder 注意力<pad>占位符掩码\n",
    "    # shape = [batch size, src len]\n",
    "    dec_enc_attn_mask = get_attn_pad_mask(trg, src, src_pad_idx)\n",
    "\n",
    "    return enc_self_attn_mask, dec_self_attn_mask, dec_enc_attn_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512  # Embedding层维度\n",
    "n_head = 8  # 多头感知机中头的数量\n",
    "n_layer = 6  # 编码器和解码器的层数\n",
    "d_ff = 2048  # 前馈神经网络维度\n",
    "max_len = 32  # 序列最大长度\n",
    "\n",
    "compute_dtype = mindspore.float32\n",
    "\n",
    "positional_encoding = PositionalEncoding(d_model, max_len=max_len + 1)  # 位置掩码\n",
    "src_content_emb = nn.Embedding(len(de_vocab), d_model)  # 源序列embedding\n",
    "trg_content_emb = nn.Embedding(len(en_vocab), d_model)  # 目标序列embedding\n",
    "src_pad_idx = de_vocab.pad_idx  # 源序列中<pad>占位符索引\n",
    "trg_pad_idx = en_vocab.pad_idx  # 目标序列中<pad>占位符索引\n",
    "\n",
    "model = nn.Transformer(d_model=d_model,\n",
    "                       nhead=n_head,\n",
    "                       num_encoder_layers=n_layer,\n",
    "                       num_decoder_layers=n_layer,\n",
    "                       dim_feedforward=d_ff,\n",
    "                       batch_first=True)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
    "opt = nn.Adam(model.trainable_params(), learning_rate=0.0001)\n",
    "\n",
    "\n",
    "def forward_fn(src,\n",
    "               trg,\n",
    "               src_mask=None,\n",
    "               trg_mask=None,\n",
    "               memory_mask=None,\n",
    "               src_key_padding_mask=None,\n",
    "               memory_key_padding_mask=None,\n",
    "               trg_key_padding_mask=None):\n",
    "    \"\"\"前向网络\"\"\"\n",
    "    output = model(src, trg, src_mask, trg_mask, memory_mask,\n",
    "                   src_key_padding_mask, memory_key_padding_mask,\n",
    "                   trg_key_padding_mask)\n",
    "    loss = loss_fn(output, trg)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# 反向传播计算梯度\n",
    "grad_fn = mindspore.value_and_grad(forward_fn, None, opt.parameters)\n",
    "\n",
    "\n",
    "def train_step(src,\n",
    "               trg,\n",
    "               src_mask=None,\n",
    "               trg_mask=None,\n",
    "               memory_mask=None,\n",
    "               src_key_padding_mask=None,\n",
    "               memory_key_padding_mask=None,\n",
    "               trg_key_padding_mask=None):\n",
    "    \"\"\"单步训练\"\"\"\n",
    "    # 计算损失与梯度\n",
    "    loss, grads = grad_fn(src, trg, src_mask, trg_mask, memory_mask,\n",
    "                          src_key_padding_mask, memory_key_padding_mask,\n",
    "                          trg_key_padding_mask)\n",
    "    # 更新网络权重\n",
    "    opt(grads)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterator, epoch=0):\n",
    "    \"\"\"模型训练\"\"\"\n",
    "    model.set_train(True)\n",
    "    num_batches = len(iterator)\n",
    "    total_loss = 0  # 所有batch训练loss的累加\n",
    "    total_steps = 0  # 训练步数\n",
    "\n",
    "    with tqdm(total=num_batches) as t:\n",
    "        t.set_description(f'Epoch: {epoch}')\n",
    "        for src, src_len, trg in iterator():\n",
    "            # 计算源序列与目标序列的embedding，并加入位置编码\n",
    "            # 为平衡原embedding和位置编码的数值，将原embedding以适当比例放大\n",
    "            content_scale = ops.sqrt(\n",
    "                mindspore.Tensor([d_model], mindspore.float32))\n",
    "            src_emb = positional_encoding(src_content_emb(src) * content_scale)\n",
    "            trg_emb = positional_encoding(trg_content_emb(trg) * content_scale)\n",
    "\n",
    "            # encoder与decoder中的掩码\n",
    "            # 将dec_self_attn_mask的shape变为[batch size * n_head, trg len, trg len]\n",
    "            enc_self_attn_mask, dec_self_attn_mask, dec_enc_attn_mask = get_enc_dec_mask(\n",
    "                src, trg, src_pad_idx, trg_pad_idx)\n",
    "            dec_self_attn_mask = ops.tile(dec_self_attn_mask, (n_head, 1, 1))\n",
    "\n",
    "            # 计算当前batch数据的loss\n",
    "            loss = train_step(src_emb,\n",
    "                              trg_emb,\n",
    "                              src_key_padding_mask=enc_self_attn_mask,\n",
    "                              memory_key_padding_mask=dec_enc_attn_mask,\n",
    "                              trg_mask=dec_self_attn_mask)\n",
    "            total_loss += loss.asnumpy()\n",
    "            total_steps += 1\n",
    "            # 当前的平均loss\n",
    "            curr_loss = total_loss / total_steps\n",
    "            t.set_postfix({'loss': f'{curr_loss:.2f}'})\n",
    "            t.update(1)\n",
    "\n",
    "    return total_loss / total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for src, src_len, trg in test_iterator():\n",
    "    # 计算源序列与目标序列的embedding，并加入位置编码\n",
    "    # 为平衡原embedding和位置编码的数值，将原embedding以适当比例放大\n",
    "    content_scale = ops.sqrt(mindspore.Tensor([d_model], mindspore.float32))\n",
    "    src_emb = positional_encoding(src_content_emb(src) * content_scale)\n",
    "    trg_emb = positional_encoding(trg_content_emb(trg) * content_scale)\n",
    "\n",
    "    # encoder与decoder中的掩码\n",
    "    # 将dec_self_attn_mask的shape变为[batch size * n_head, trg len, trg len]\n",
    "    enc_self_attn_mask, dec_self_attn_mask, dec_enc_attn_mask = get_enc_dec_mask(\n",
    "        src, trg, src_pad_idx, trg_pad_idx)\n",
    "    dec_self_attn_mask = ops.tile(dec_self_attn_mask, (n_head, 1, 1))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(iterator):\n",
    "    \"\"\"模型验证\"\"\"\n",
    "    model.set_train(False)\n",
    "    num_batches = len(iterator)\n",
    "    total_loss = 0 # 所有batch训练loss的累加\n",
    "    total_steps = 0 # 训练步数\n",
    "    \n",
    "    with tqdm(total=num_batches) as t:\n",
    "        for src, src_len, trg in iterator():\n",
    "            src_emb = embedding(src, de_vocab, max_len, d_model)\n",
    "            trg_emb = embedding(trg, en_vocab, max_len, d_model)\n",
    "            loss = forward_fn(src_emb, trg_emb) # 当前batch的loss\n",
    "            total_loss += loss.asnumpy()\n",
    "            total_steps += 1\n",
    "            curr_loss = total_loss / total_steps # 当前的平均loss\n",
    "            t.set_postfix({'loss': f'{curr_loss:.2f}'})\n",
    "            t.update(1)\n",
    "    \n",
    "    return total_loss / total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import mindspore\n",
    "import mindspore.ops as ops\n",
    "import mindspore.numpy as mnp\n",
    "import numpy as np\n",
    "from mindspore import Tensor\n",
    "max_len = 32\n",
    "ones = ops.ones((max_len, max_len), mindspore.float32)\n",
    "subsequent_mask = ones.triu(diagonal=1)\n",
    "dec_self_attn_subsequent_mask = get_attn_subsequent_mask(subsequent_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add & Norm\n",
    "\n",
    "- Add\n",
    "- Norm： Layer Norm\n",
    "\n",
    "### 前馈神经网络（Feed-Forward Nerual Network，FFN）\n",
    "\n",
    "- Pointwise FFN\n",
    "\n",
    "### 编码器（Encoder）\n",
    "\n",
    "### 解码器（Decoder）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "\n",
    "d_model = 512  # Embedding层维度\n",
    "n_head = 8  # 多头感知机中头的数量\n",
    "n_layer = 6  # 编码器和解码器的层数\n",
    "d_ff = 2048  # 前馈神经网络维度\n",
    "max_len = 32  # 序列最大长度\n",
    "\n",
    "\n",
    "model = nn.Transformer()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = nn.Adam(model.trainable_params(), learning_rate=0.0001)\n",
    "\n",
    "def forward_fn(src, trg):\n",
    "    output = model(src, trg)\n",
    "    loss = loss_fn(output, trg)\n",
    "\n",
    "    return loss\n",
    "\n",
    "grad_fn = mindspore.value_and_grad(forward_fn, None, opt.parameters)\n",
    "\n",
    "def train_step(src, trg):\n",
    "    loss, grads = grad_fn(src, trg)\n",
    "    opt(grads)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False ...  True  True  True]\n",
      " [False False False ...  True  True  True]\n",
      " [False False False ...  True  True  True]\n",
      " ...\n",
      " [False False False ...  True  True  True]\n",
      " [False False False ...  True  True  True]\n",
      " [False False False ...  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "for src, src_len, trg in test_iterator():\n",
    "    mask = ops.equal(src, 1)\n",
    "    print(mask)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "For 'Add', the type of 'x' must be one of Tensor[Int8], Tensor[Int16], Tensor[Int32], Tensor[Int64], Tensor[UInt8], Tensor[UInt16], Tensor[UInt32], Tensor[UInt64], Tensor[Float16], Tensor[Float32], Tensor[Float64], Tensor[Complex64], Tensor[Complex128], but got Tensor[Bool].The supported data types depend on the hardware that executes the operator, for more details, please refer to the MindSpore official website to get more information about the data type.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [45], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m dec_self_attn_pad_mask \u001b[39m=\u001b[39m get_attn_pad_mask(trg, trg, \u001b[39m1\u001b[39m)\n\u001b[0;32m     20\u001b[0m dec_self_attn_subsequent_mask \u001b[39m=\u001b[39m get_attn_subsequent_mask(dec_self_attn_pad_mask)\n\u001b[1;32m---> 21\u001b[0m dec_self_attn_mask \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mgt((dec_self_attn_pad_mask \u001b[39m+\u001b[39;49m dec_self_attn_subsequent_mask), \u001b[39m0\u001b[39m)\n\u001b[0;32m     23\u001b[0m dec_enc_attn_mask \u001b[39m=\u001b[39m get_attn_pad_mask(trg, src, \u001b[39m1\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[39mprint\u001b[39m(dec_self_attn_mask\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\common\\tensor.py:325\u001b[0m, in \u001b[0;36mTensor.__add__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__add__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[1;32m--> 325\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor_operator_registry\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39m__add__\u001b[39;49m\u001b[39m'\u001b[39;49m)(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\composite\\multitype_ops\\_compile_utils.py:103\u001b[0m, in \u001b[0;36m_tensor_add\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, COOTensor):\n\u001b[0;32m    102\u001b[0m     \u001b[39mreturn\u001b[39;00m other \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49madd(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\function\\math_func.py:308\u001b[0m, in \u001b[0;36madd\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd\u001b[39m(x, y):\n\u001b[0;32m    257\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[39m    Adds two input tensors element-wise.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[39m        Float32\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor_add(x, y)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\primitive.py:317\u001b[0m, in \u001b[0;36mPrimitive.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39mif\u001b[39;00m should_elim:\n\u001b[0;32m    316\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m _run_op(\u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname, args)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\primitive.py:885\u001b[0m, in \u001b[0;36m_run_op\u001b[1;34m(obj, op_name, args)\u001b[0m\n\u001b[0;32m    883\u001b[0m     stub \u001b[39m=\u001b[39m _pynative_executor\u001b[39m.\u001b[39mrun_op_async(obj, args)\n\u001b[0;32m    884\u001b[0m     \u001b[39mreturn\u001b[39;00m _convert_stub(stub)\n\u001b[1;32m--> 885\u001b[0m \u001b[39mreturn\u001b[39;00m _run_op_sync(obj, op_name, args)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\common\\api.py:101\u001b[0m, in \u001b[0;36m_wrap_func.<locals>.wrapper\u001b[1;34m(*arg, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39m@wraps\u001b[39m(fn)\n\u001b[0;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39marg, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 101\u001b[0m     results \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49marg, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    102\u001b[0m     \u001b[39mreturn\u001b[39;00m _convert_python_data(results)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\primitive.py:891\u001b[0m, in \u001b[0;36m_run_op_sync\u001b[1;34m(obj, op_name, args)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[39m@_wrap_func\u001b[39m\n\u001b[0;32m    889\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_op_sync\u001b[39m(obj, op_name, args):\n\u001b[0;32m    890\u001b[0m     \u001b[39m\"\"\"Single op execution function in synchronous mode.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 891\u001b[0m     output \u001b[39m=\u001b[39m _pynative_executor\u001b[39m.\u001b[39;49mreal_run_op(obj, op_name, args)\n\u001b[0;32m    892\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\common\\api.py:1039\u001b[0m, in \u001b[0;36m_PyNativeExecutor.real_run_op\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreal_run_op\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[0;32m   1030\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m \u001b[39m    Run single op.\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[39m        Tensor, result of run op.\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1039\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_executor\u001b[39m.\u001b[39;49mreal_run_op(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\primitive.py:642\u001b[0m, in \u001b[0;36mPrimitiveWithInfer.__infer__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    640\u001b[0m     fn \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minfer_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m track)\n\u001b[0;32m    641\u001b[0m     \u001b[39m# fn may return None\u001b[39;00m\n\u001b[1;32m--> 642\u001b[0m     out[track] \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49m(x[track] \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m args))\n\u001b[0;32m    644\u001b[0m \u001b[39m# output does not contain dynamic shape, no need to calculate min/max shape\u001b[39;00m\n\u001b[0;32m    646\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhas_dynamic_shape\u001b[39m(shp):\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\operations\\math_ops.py:115\u001b[0m, in \u001b[0;36m_MathBinaryOp.infer_dtype\u001b[1;34m(self, x_dtype, y_dtype)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minfer_dtype\u001b[39m(\u001b[39mself\u001b[39m, x_dtype, y_dtype):\n\u001b[1;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m _MathBinaryOp\u001b[39m.\u001b[39;49mdo_infer_dtype(x_dtype, y_dtype, mstype\u001b[39m.\u001b[39;49mnumber_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\operations\\math_ops.py:111\u001b[0m, in \u001b[0;36m_MathBinaryOp.do_infer_dtype\u001b[1;34m(x_dtype, y_dtype, valid_dtype, prim_name)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mComplex math binary op expecting Tensor [Complex64, Complex64],\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    106\u001b[0m                         \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m[Complex64, Float32], [Float32, Complex64], [Complex128, Complex128],\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    107\u001b[0m                         \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m[Complex128, Float64], [Float64, Complex128],\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    108\u001b[0m                         \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbut got : [\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mformat\u001b[39m(x_dtype)\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mformat\u001b[39m(y_dtype)\u001b[39m}\u001b[39;00m\u001b[39m].\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    109\u001b[0m     \u001b[39mreturn\u001b[39;00m type_infer_dict\u001b[39m.\u001b[39mget((x_dtype\u001b[39m.\u001b[39melement_type(), y_dtype\u001b[39m.\u001b[39melement_type()))\n\u001b[1;32m--> 111\u001b[0m validator\u001b[39m.\u001b[39;49mcheck_tensors_dtypes_same_and_valid(args_type, valid_dtype, prim_name)\n\u001b[0;32m    112\u001b[0m \u001b[39mreturn\u001b[39;00m x_dtype\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\_checkparam.py:682\u001b[0m, in \u001b[0;36mValidator.check_tensors_dtypes_same_and_valid\u001b[1;34m(args, valid_dtypes, prim_name)\u001b[0m\n\u001b[0;32m    680\u001b[0m valid_dtypes \u001b[39m=\u001b[39m valid_dtypes \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(valid_dtypes, Iterable) \u001b[39melse\u001b[39;00m [valid_dtypes]\n\u001b[0;32m    681\u001b[0m tensor_types \u001b[39m=\u001b[39m [mstype\u001b[39m.\u001b[39mtensor_type(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m valid_dtypes]\n\u001b[1;32m--> 682\u001b[0m Validator\u001b[39m.\u001b[39;49mcheck_types_same_and_valid(args, tensor_types, prim_name)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\_checkparam.py:675\u001b[0m, in \u001b[0;36mValidator.check_types_same_and_valid\u001b[1;34m(args, valid_values, prim_name)\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[39mreturn\u001b[39;00m arg1\n\u001b[0;32m    674\u001b[0m elem_types \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(_check_type_valid, args\u001b[39m.\u001b[39mitems())\n\u001b[1;32m--> 675\u001b[0m reduce(_check_types_same, elem_types)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\_checkparam.py:662\u001b[0m, in \u001b[0;36mValidator.check_types_same_and_valid.<locals>._check_type_valid\u001b[1;34m(arg)\u001b[0m\n\u001b[0;32m    660\u001b[0m arg_key, arg_val \u001b[39m=\u001b[39m arg\n\u001b[0;32m    661\u001b[0m elem_type \u001b[39m=\u001b[39m arg_val\n\u001b[1;32m--> 662\u001b[0m Validator\u001b[39m.\u001b[39;49mcheck_subclass(arg_key, elem_type, valid_values, prim_name)\n\u001b[0;32m    663\u001b[0m \u001b[39mreturn\u001b[39;00m (arg_key, elem_type)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\_checkparam.py:641\u001b[0m, in \u001b[0;36mValidator.check_subclass\u001b[1;34m(arg_name, type_, template_types, prim_name, addition_error_info)\u001b[0m\n\u001b[0;32m    639\u001b[0m     addition_error_info \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m addition_error_info\n\u001b[0;32m    640\u001b[0m type_str \u001b[39m=\u001b[39m (\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtype \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(type_)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(type_, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m)) \u001b[39melse\u001b[39;00m \u001b[39mstr\u001b[39m(type_))\n\u001b[1;32m--> 641\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFor \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mprim_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, the type of \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00marg_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    642\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m must be \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39mone of \u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(template_types) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    643\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin((\u001b[39mstr\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m template_types))\u001b[39m}\u001b[39;00m\u001b[39m, but got \u001b[39m\u001b[39m{\u001b[39;00mtype_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    644\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00maddition_error_info\u001b[39m}\u001b[39;00m\u001b[39m.The supported data types depend on the hardware that\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    645\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m executes the operator, for more details, please refer to the MindSpore official \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    646\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwebsite to get more information about the data type.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: For 'Add', the type of 'x' must be one of Tensor[Int8], Tensor[Int16], Tensor[Int32], Tensor[Int64], Tensor[UInt8], Tensor[UInt16], Tensor[UInt32], Tensor[UInt64], Tensor[Float16], Tensor[Float32], Tensor[Float64], Tensor[Complex64], Tensor[Complex128], but got Tensor[Bool].The supported data types depend on the hardware that executes the operator, for more details, please refer to the MindSpore official website to get more information about the data type."
     ]
    }
   ],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k, pad_idx):\n",
    "    # print(seq_q.shape)\n",
    "    batch_size, len_q = seq_q.shape\n",
    "    batch_size, len_k = seq_k.shape\n",
    "\n",
    "    pad_attn_mask = ops.equal(seq_k, pad_idx)\n",
    "    pad_attn_mask = pad_attn_mask.expand_dims(1)\n",
    "\n",
    "    return ops.broadcast_to(pad_attn_mask, (batch_size, len_q, len_k))\n",
    "\n",
    "def get_attn_subsequent_mask(subsequent_mask):\n",
    "    subsequent_mask = subsequent_mask.expand_dims(0)\n",
    "    return subsequent_mask\n",
    "\n",
    "for src, src_len, trg in test_iterator():\n",
    "    # src_emb = nn.Embedding(len(de_vocab), d_model)(src)\n",
    "    # trg_emb = nn.Embedding(len(en_vocab), d_model)(trg)\n",
    "\n",
    "    dec_self_attn_pad_mask = get_attn_pad_mask(trg, trg, 1)\n",
    "    dec_self_attn_subsequent_mask = get_attn_subsequent_mask(dec_self_attn_pad_mask)\n",
    "    dec_self_attn_mask = ops.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\n",
    "\n",
    "    dec_enc_attn_mask = get_attn_pad_mask(trg, src, 1)\n",
    "\n",
    "    print(dec_self_attn_mask.shape)\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.ops.function.nn_func import multi_head_attention_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 512)\n",
      "(128, 32, 512)\n"
     ]
    }
   ],
   "source": [
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore import Tensor\n",
    "\n",
    "d_model = 512  # Embedding层维度\n",
    "n_head = 8  # 多头感知机中头的数量\n",
    "n_layer = 6  # 编码器和解码器的层数\n",
    "d_ff = 2048  # 前馈神经网络维度\n",
    "max_len = 32  # 序列最大长度\n",
    "\n",
    "compute_dtype = mindspore.float32\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Cell):\n",
    "    \"\"\"位置编码\"\"\"\n",
    "    def __init__(self, embed_dim, dropout=0.1, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # 位置信息\n",
    "        # shape = [1, max len, embed dim]\n",
    "        self.pos = ops.fill(compute_dtype, (1, max_len, embed_dim), 0)\n",
    "        angle = ops.arange(end=max_len, dtype=compute_dtype).reshape(\n",
    "            -1, 1) / ops.pow(\n",
    "                10000,\n",
    "                ops.arange(end=embed_dim, step=2, dtype=compute_dtype) /\n",
    "                embed_dim)\n",
    "        self.pos[:, :, 0::2] = ops.sin(angle)\n",
    "        self.pos[:, :, 1::2] = ops.cos(angle)\n",
    "\n",
    "    def construct(self, x):\n",
    "        # 将位置编码截取至x同等大小\n",
    "        x = x + self.pos[:, :x.shape[1], :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "def get_key_padding_mask()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 512)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from mindspore import Tensor\n",
    "import numpy as np\n",
    "\n",
    "def get_sinusoid_encoding_table(n_position, d_model):\n",
    "    def cal_angle(position, hid_idx):\n",
    "        return position / np.power(10000, 2 * (hid_idx // 2) / d_model)\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "    return Tensor(sinusoid_table, mindspore.float32)\n",
    "\n",
    "\n",
    "def embedding(input, vocab, max_len, d_model):\n",
    "    tok_emb = nn.Embedding(len(vocab), d_model)\n",
    "    sinusoid_table = get_sinusoid_encoding_table(max_len, d_model)\n",
    "    pos_emb = nn.Embedding(sinusoid_table.shape[0], sinusoid_table.shape[1], embedding_table=sinusoid_table)\n",
    "    output = tok_emb(input) + pos_emb(input)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterator, epoch=0):\n",
    "    \"\"\"模型训练\"\"\"\n",
    "    model.set_train(True)\n",
    "    num_batches = len(iterator)\n",
    "    total_loss = 0 # 所有batch训练loss的累加\n",
    "    total_steps = 0 # 训练步数\n",
    "\n",
    "    with tqdm(total=num_batches) as t:\n",
    "        t.set_description(f'Epoch: {epoch}')\n",
    "        for src, src_len, trg in iterator():\n",
    "            src_emb = embedding(src, de_vocab, max_len, d_model)\n",
    "            trg_emb = embedding(trg, en_vocab, max_len, d_model)\n",
    "            # print(src_emb.shape)\n",
    "            # print(trg_emb.shape)\n",
    "            loss = train_step(src_emb, trg_emb) # 当前batch的loss\n",
    "            total_loss += loss.asnumpy()\n",
    "            total_steps += 1\n",
    "            curr_loss = total_loss / total_steps # 当前的平均loss\n",
    "            t.set_postfix({'loss': f'{curr_loss:.2f}'})\n",
    "            t.update(1)\n",
    "    \n",
    "    return total_loss / total_steps\n",
    "\n",
    "\n",
    "def evaluate(iterator):\n",
    "    \"\"\"模型验证\"\"\"\n",
    "    model.set_train(False)\n",
    "    num_batches = len(iterator)\n",
    "    total_loss = 0 # 所有batch训练loss的累加\n",
    "    total_steps = 0 # 训练步数\n",
    "    \n",
    "    with tqdm(total=num_batches) as t:\n",
    "        for src, src_len, trg in iterator():\n",
    "            src_emb = embedding(src, de_vocab, max_len, d_model)\n",
    "            trg_emb = embedding(trg, en_vocab, max_len, d_model)\n",
    "            loss = forward_fn(src_emb, trg_emb) # 当前batch的loss\n",
    "            total_loss += loss.asnumpy()\n",
    "            total_steps += 1\n",
    "            curr_loss = total_loss / total_steps # 当前的平均loss\n",
    "            t.set_postfix({'loss': f'{curr_loss:.2f}'})\n",
    "            t.update(1)\n",
    "    \n",
    "    return total_loss / total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0: 100%|██████████| 226/226 [4:21:31<00:00, 69.43s/it, loss=41.43]     \n",
      "100%|██████████| 8/8 [00:58<00:00,  7.28s/it, loss=40.79]\n",
      "Epoch: 1: 100%|██████████| 226/226 [11:25:32<00:00, 182.00s/it, loss=40.83]    \n",
      "100%|██████████| 8/8 [00:32<00:00,  4.11s/it, loss=40.69]\n",
      "Epoch: 2: 100%|██████████| 226/226 [50:49<00:00, 13.49s/it, loss=40.71]\n",
      "100%|██████████| 8/8 [00:26<00:00,  3.33s/it, loss=40.62]\n",
      "Epoch: 3: 100%|██████████| 226/226 [50:12<00:00, 13.33s/it, loss=40.60]\n",
      "100%|██████████| 8/8 [00:29<00:00,  3.68s/it, loss=40.54]\n",
      "Epoch: 4:   6%|▌         | 14/226 [03:12<46:59, 13.30s/it, loss=40.80] "
     ]
    }
   ],
   "source": [
    "from mindspore import save_checkpoint\n",
    "\n",
    "num_epochs = 10 # 训练迭代数\n",
    "clip = 1.0 # 梯度裁剪阈值\n",
    "best_valid_loss = float('inf') # 当前最佳验证损失\n",
    "ckpt_file_name = os.path.join(cache_dir, 'transformer.ckpt') # 模型保存路径\n",
    "\n",
    "# mindspore.set_context(mode=mindspore.PYNATIVE_MODE)\n",
    "mindspore.set_context(mode=mindspore.GRAPH_MODE)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    # 模型训练，网络权重更新\n",
    "    train_loss = train(train_iterator, i)\n",
    "    # 网络权重更新后对模型进行验证\n",
    "    valid_loss = evaluate(valid_iterator)\n",
    "    \n",
    "    # 保存当前效果最好的模型\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        save_checkpoint(model, ckpt_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_model: 512\n",
      "attn type after softmax:  <class 'mindspore.common.tensor.Tensor'>\n",
      "attn type after dropout: <class 'mindspore.common.tensor.Tensor'>\n",
      "attn type after softmax:  <class 'mindspore.common.tensor.Tensor'>\n",
      "attn type after dropout: <class 'mindspore.common.tensor.Tensor'>\n",
      "attn type after softmax:  <class 'mindspore.common.tensor.Tensor'>\n",
      "attn type after dropout: <class 'mindspore.common.tensor.Tensor'>\n",
      "attn type after softmax:  <class 'mindspore.common.tensor.Tensor'>\n",
      "attn type after dropout: <class 'mindspore.common.tensor.Tensor'>\n",
      "attn type after softmax:  <class 'mindspore.common.tensor.Tensor'>\n",
      "attn type after dropout: <class 'mindspore.common.tensor.Tensor'>\n",
      "attn type after softmax:  <class 'mindspore.common.tensor.Tensor'>\n",
      "attn type after dropout: <class 'mindspore.common.tensor.Tensor'>\n",
      "attn type after softmax:  <class 'mindspore.common.tensor.Tensor'>\n",
      "attn type after dropout: <class 'mindspore.common.tensor.Tensor'>\n",
      "attn type after softmax:  <class 'mindspore.common.tensor.Tensor'>\n",
      "attn type after dropout: <class 'mindspore.common.tensor.Tensor'>\n",
      "attn type after softmax:  <class 'mindspore.common.tensor.Tensor'>\n",
      "attn type after dropout: <class 'mindspore.common.tensor.Tensor'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [13], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m loss\n\u001b[0;32m     21\u001b[0m net_work\u001b[39m.\u001b[39mset_train(\u001b[39mTrue\u001b[39;00m)   \n\u001b[1;32m---> 22\u001b[0m train_step(src, trg)\n",
      "Cell \u001b[1;32mIn [13], line 17\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(src, trg)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_step\u001b[39m(src, trg):\n\u001b[1;32m---> 17\u001b[0m     loss, grads \u001b[39m=\u001b[39m grad_fn(src, trg)\n\u001b[0;32m     18\u001b[0m     optimizer(grads)\n\u001b[0;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\composite\\base.py:605\u001b[0m, in \u001b[0;36m_Grad.__call__.<locals>.after_grad\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mafter_grad\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 605\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_(fn_, weights)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\common\\api.py:101\u001b[0m, in \u001b[0;36m_wrap_func.<locals>.wrapper\u001b[1;34m(*arg, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39m@wraps\u001b[39m(fn)\n\u001b[0;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39marg, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 101\u001b[0m     results \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49marg, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    102\u001b[0m     \u001b[39mreturn\u001b[39;00m _convert_python_data(results)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\composite\\base.py:582\u001b[0m, in \u001b[0;36m_Grad.__call__.<locals>.after_grad\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[39m@_wrap_func\u001b[39m\n\u001b[0;32m    581\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mafter_grad\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 582\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pynative_forward_run(fn, grad_, args, kwargs)\n\u001b[0;32m    583\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mgrad(fn, grad_, weights, grad_position, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    584\u001b[0m     out \u001b[39m=\u001b[39m _pynative_executor()\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\composite\\base.py:631\u001b[0m, in \u001b[0;36m_Grad._pynative_forward_run\u001b[1;34m(self, fn, grad, args, kwargs)\u001b[0m\n\u001b[0;32m    629\u001b[0m _pynative_executor\u001b[39m.\u001b[39mset_grad_flag(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    630\u001b[0m _pynative_executor\u001b[39m.\u001b[39mnew_graph(fn, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnew_kwargs)\n\u001b[1;32m--> 631\u001b[0m outputs \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_kwargs)\n\u001b[0;32m    632\u001b[0m _pynative_executor\u001b[39m.\u001b[39mend_graph(fn, outputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnew_kwargs)\n\u001b[0;32m    633\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "Cell \u001b[1;32mIn [13], line 9\u001b[0m, in \u001b[0;36mforward_fn\u001b[1;34m(src, trg)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_fn\u001b[39m(src, trg):\n\u001b[1;32m----> 9\u001b[0m     out \u001b[39m=\u001b[39m net_work(src, trg)\n\u001b[0;32m     10\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(out, trg)\n\u001b[0;32m     11\u001b[0m     \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:653\u001b[0m, in \u001b[0;36mCell.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    652\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mnew_graph(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 653\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_construct(args, kwargs)\n\u001b[0;32m    654\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mend_graph(\u001b[39mself\u001b[39m, output, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:441\u001b[0m, in \u001b[0;36mCell._run_construct\u001b[1;34m(self, cast_inputs, kwargs)\u001b[0m\n\u001b[0;32m    439\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shard_fn(\u001b[39m*\u001b[39mcast_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstruct(\u001b[39m*\u001b[39;49mcast_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    442\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_forward_hook:\n\u001b[0;32m    443\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_forward_hook(cast_inputs, output)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\layer\\transformer.py:630\u001b[0m, in \u001b[0;36mTransformer.construct\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    627\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    629\u001b[0m memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(src, mask\u001b[39m=\u001b[39msrc_mask, src_key_padding_mask\u001b[39m=\u001b[39msrc_key_padding_mask)\n\u001b[1;32m--> 630\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(tgt, memory, tgt_mask\u001b[39m=\u001b[39;49mtgt_mask, memory_mask\u001b[39m=\u001b[39;49mmemory_mask,\n\u001b[0;32m    631\u001b[0m                       tgt_key_padding_mask\u001b[39m=\u001b[39;49mtgt_key_padding_mask,\n\u001b[0;32m    632\u001b[0m                       memory_key_padding_mask\u001b[39m=\u001b[39;49mmemory_key_padding_mask)\n\u001b[0;32m    633\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:653\u001b[0m, in \u001b[0;36mCell.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    652\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mnew_graph(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 653\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_construct(args, kwargs)\n\u001b[0;32m    654\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mend_graph(\u001b[39mself\u001b[39m, output, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:441\u001b[0m, in \u001b[0;36mCell._run_construct\u001b[1;34m(self, cast_inputs, kwargs)\u001b[0m\n\u001b[0;32m    439\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shard_fn(\u001b[39m*\u001b[39mcast_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstruct(\u001b[39m*\u001b[39;49mcast_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    442\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_forward_hook:\n\u001b[0;32m    443\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_forward_hook(cast_inputs, output)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\layer\\transformer.py:532\u001b[0m, in \u001b[0;36mTransformerDecoder.construct\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    529\u001b[0m output \u001b[39m=\u001b[39m tgt\n\u001b[0;32m    531\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> 532\u001b[0m     output \u001b[39m=\u001b[39m mod(output, memory, tgt_mask\u001b[39m=\u001b[39;49mtgt_mask,\n\u001b[0;32m    533\u001b[0m                  memory_mask\u001b[39m=\u001b[39;49mmemory_mask,\n\u001b[0;32m    534\u001b[0m                  tgt_key_padding_mask\u001b[39m=\u001b[39;49mtgt_key_padding_mask,\n\u001b[0;32m    535\u001b[0m                  memory_key_padding_mask\u001b[39m=\u001b[39;49mmemory_key_padding_mask)\n\u001b[0;32m    537\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    538\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(output)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:653\u001b[0m, in \u001b[0;36mCell.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    652\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mnew_graph(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 653\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_construct(args, kwargs)\n\u001b[0;32m    654\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mend_graph(\u001b[39mself\u001b[39m, output, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:441\u001b[0m, in \u001b[0;36mCell._run_construct\u001b[1;34m(self, cast_inputs, kwargs)\u001b[0m\n\u001b[0;32m    439\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shard_fn(\u001b[39m*\u001b[39mcast_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstruct(\u001b[39m*\u001b[39;49mcast_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    442\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_forward_hook:\n\u001b[0;32m    443\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_forward_hook(cast_inputs, output)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\layer\\transformer.py:409\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.construct\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    407\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm3(x))\n\u001b[0;32m    408\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, tgt_mask, tgt_key_padding_mask))\n\u001b[0;32m    410\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mha_block(x, memory, memory_mask, memory_key_padding_mask))\n\u001b[0;32m    411\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm3(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\layer\\transformer.py:416\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x, attn_mask, key_padding_mask):\n\u001b[1;32m--> 416\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[0;32m    417\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    418\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m    419\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    420\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\layer\\transformer.py:177\u001b[0m, in \u001b[0;36mMultiheadAttention.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_is_v \u001b[39m=\u001b[39m key \u001b[39mis\u001b[39;00m value\n\u001b[0;32m    176\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_is_k \u001b[39m=\u001b[39m query \u001b[39mis\u001b[39;00m key\n\u001b[1;32m--> 177\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:653\u001b[0m, in \u001b[0;36mCell.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    652\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mnew_graph(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 653\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_construct(args, kwargs)\n\u001b[0;32m    654\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mend_graph(\u001b[39mself\u001b[39m, output, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:441\u001b[0m, in \u001b[0;36mCell._run_construct\u001b[1;34m(self, cast_inputs, kwargs)\u001b[0m\n\u001b[0;32m    439\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shard_fn(\u001b[39m*\u001b[39mcast_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstruct(\u001b[39m*\u001b[39;49mcast_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    442\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_forward_hook:\n\u001b[0;32m    443\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_forward_hook(cast_inputs, output)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\layer\\transformer.py:212\u001b[0m, in \u001b[0;36mMultiheadAttention.construct\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[0;32m    200\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m multi_head_attention_forward(\n\u001b[0;32m    201\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[0;32m    202\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight, average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[0;32m    210\u001b[0m         k_is_v\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_is_v, q_is_k\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_is_k)\n\u001b[0;32m    211\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 212\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m multi_head_attention_forward(\n\u001b[0;32m    213\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[0;32m    214\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[0;32m    215\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[0;32m    216\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    217\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[0;32m    218\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m    219\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask, average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[0;32m    220\u001b[0m         k_is_v\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mk_is_v, q_is_k\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_is_k)\n\u001b[0;32m    222\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[0;32m    223\u001b[0m     attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mswapaxes(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\function\\nn_func.py:6046\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal, k_is_v, q_is_k)\u001b[0m\n\u001b[0;32m   6043\u001b[0m k \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m   6044\u001b[0m v \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[1;32m-> 6046\u001b[0m attn_output, attn_output_weights \u001b[39m=\u001b[39m _scaled_dot_product_attention(\n\u001b[0;32m   6047\u001b[0m     q, k, v, attn_mask, dropout_p, is_causal, training)\n\u001b[0;32m   6048\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mview(bsz \u001b[39m*\u001b[39m tgt_len, embed_dim)\n\u001b[0;32m   6050\u001b[0m attn_output \u001b[39m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\function\\nn_func.py:5845\u001b[0m, in \u001b[0;36m_scaled_dot_product_attention\u001b[1;34m(query, key, value, attn_mask, dropout_p, is_causal, is_training)\u001b[0m\n\u001b[0;32m   5843\u001b[0m     attn, _ \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mdropout(attn, dropout_p)\n\u001b[0;32m   5844\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mattn type after dropout:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m(attn))\n\u001b[1;32m-> 5845\u001b[0m output \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mmatmul(attn, value)\n\u001b[0;32m   5847\u001b[0m \u001b[39mreturn\u001b[39;00m (output, attn)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\function\\math_func.py:7908\u001b[0m, in \u001b[0;36mmatmul\u001b[1;34m(x1, x2)\u001b[0m\n\u001b[0;32m   7906\u001b[0m x2 \u001b[39m=\u001b[39m _expand(x2, ndim_aligned)\n\u001b[0;32m   7907\u001b[0m shape1_aligned, shape2_aligned \u001b[39m=\u001b[39m shape_op(x1), shape_op(x2)\n\u001b[1;32m-> 7908\u001b[0m x1 \u001b[39m=\u001b[39m _broadcast_to(x1, shape1_aligned[:\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m], shape_backbone, ndim_aligned)\n\u001b[0;32m   7909\u001b[0m x2 \u001b[39m=\u001b[39m _broadcast_to(x2, shape2_aligned[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m], shape_backbone, ndim_aligned)\n\u001b[0;32m   7910\u001b[0m res \u001b[39m=\u001b[39m _batch_matmul(x1, x2)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\function\\math_func.py:7818\u001b[0m, in \u001b[0;36m_broadcast_to\u001b[1;34m(x, shape_cur, shape_to, ndim_to)\u001b[0m\n\u001b[0;32m   7816\u001b[0m size \u001b[39m=\u001b[39m tile_size_op(shape_cur, shape_to, ndim_to)\n\u001b[0;32m   7817\u001b[0m F\u001b[39m.\u001b[39mstop_gradient(size)\n\u001b[1;32m-> 7818\u001b[0m \u001b[39mreturn\u001b[39;00m tile_op(x, size)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\primitive.py:311\u001b[0m, in \u001b[0;36mPrimitive.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[1;32m--> 311\u001b[0m     should_elim, output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_elim(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    312\u001b[0m     \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args:\n\u001b[0;32m    313\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arg, Parameter) \u001b[39mand\u001b[39;00m arg\u001b[39m.\u001b[39mhas_init:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\operations\\array_ops.py:2152\u001b[0m, in \u001b[0;36mTile.check_elim\u001b[1;34m(self, base_tensor, multiplier)\u001b[0m\n\u001b[0;32m   2148\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFor \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, the type of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mmultiplier\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be tuple, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2149\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(multiplier)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2151\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(v \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m multiplier) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(base_tensor\u001b[39m.\u001b[39mshape) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(multiplier):\n\u001b[1;32m-> 2152\u001b[0m     ret \u001b[39m=\u001b[39m Identity()(base_tensor)\n\u001b[0;32m   2153\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, ret)\n\u001b[0;32m   2154\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\primitive.py:317\u001b[0m, in \u001b[0;36mPrimitive.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39mif\u001b[39;00m should_elim:\n\u001b[0;32m    316\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m _run_op(\u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname, args)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\primitive.py:885\u001b[0m, in \u001b[0;36m_run_op\u001b[1;34m(obj, op_name, args)\u001b[0m\n\u001b[0;32m    883\u001b[0m     stub \u001b[39m=\u001b[39m _pynative_executor\u001b[39m.\u001b[39mrun_op_async(obj, args)\n\u001b[0;32m    884\u001b[0m     \u001b[39mreturn\u001b[39;00m _convert_stub(stub)\n\u001b[1;32m--> 885\u001b[0m \u001b[39mreturn\u001b[39;00m _run_op_sync(obj, op_name, args)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\common\\api.py:101\u001b[0m, in \u001b[0;36m_wrap_func.<locals>.wrapper\u001b[1;34m(*arg, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39m@wraps\u001b[39m(fn)\n\u001b[0;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39marg, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 101\u001b[0m     results \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49marg, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    102\u001b[0m     \u001b[39mreturn\u001b[39;00m _convert_python_data(results)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\primitive.py:891\u001b[0m, in \u001b[0;36m_run_op_sync\u001b[1;34m(obj, op_name, args)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[39m@_wrap_func\u001b[39m\n\u001b[0;32m    889\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_op_sync\u001b[39m(obj, op_name, args):\n\u001b[0;32m    890\u001b[0m     \u001b[39m\"\"\"Single op execution function in synchronous mode.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 891\u001b[0m     output \u001b[39m=\u001b[39m _pynative_executor\u001b[39m.\u001b[39;49mreal_run_op(obj, op_name, args)\n\u001b[0;32m    892\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\common\\api.py:1039\u001b[0m, in \u001b[0;36m_PyNativeExecutor.real_run_op\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreal_run_op\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[0;32m   1030\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m \u001b[39m    Run single op.\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[39m        Tensor, result of run op.\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1039\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_executor\u001b[39m.\u001b[39;49mreal_run_op(\u001b[39m*\u001b[39;49margs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from mindspore.ops.function.nn_func import multi_head_attention_forward\n",
    "\n",
    "net_work = nn.Transformer()\n",
    "\n",
    "src = Tensor(np.random.rand(128, 32, 512), mindspore.float32)\n",
    "trg = Tensor(np.random.rand(128, 32, 512), mindspore.float32)\n",
    "\n",
    "def forward_fn(src, trg):\n",
    "    out = net_work(src, trg)\n",
    "    loss = loss_fn(out, trg)\n",
    "    return loss\n",
    "\n",
    "optimizer = nn.Adam(net_work.trainable_params(), learning_rate=0.0001)\n",
    "grad_fn = mindspore.value_and_grad(forward_fn, None, optimizer.parameters)\n",
    "\n",
    "def train_step(src, trg):\n",
    "    loss, grads = grad_fn(src, trg)\n",
    "    optimizer(grads)\n",
    "    return loss\n",
    "\n",
    "net_work.set_train(True)   \n",
    "train_step(src, trg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_work = nn.Transformer()\n",
    "\n",
    "src = Tensor(np.random.rand(128, 32, 512), mindspore.float32)\n",
    "trg = Tensor(np.random.rand(128, 32, 512), mindspore.float32)\n",
    "\n",
    "def forward_fn(src, trg):\n",
    "    out = net_work(src, trg)\n",
    "    loss = loss_fn(out, trg)\n",
    "    return loss\n",
    "\n",
    "optimizer = nn.Adam(net_work.trainable_params(), learning_rate=0.0001)\n",
    "grad_fn = mindspore.value_and_grad(forward_fn, None, optimizer.parameters)\n",
    "\n",
    "def train_step(src, trg):\n",
    "    loss, grads = grad_fn(src, trg)\n",
    "    optimizer(grads)\n",
    "\n",
    "# net_work.set_train(True)   \n",
    "train_step(src, trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Tensor(shape=[2, 1], dtype=Int32, value=\n",
       " [[1],\n",
       "  [4]]),\n",
       " Tensor(shape=[2, 1], dtype=Int32, value=\n",
       " [[2],\n",
       "  [5]]),\n",
       " Tensor(shape=[2, 1], dtype=Int32, value=\n",
       " [[3],\n",
       "  [6]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mindspore\n",
    "from mindspore import Tensor \n",
    "\n",
    "t_test = Tensor([[1, 2, 3],[4, 5, 6]], mindspore.int32)\n",
    "print(t_test.shape)\n",
    "t_test.tensor_split(3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d735f6cc625ca8b095620eeb46a50be5e34ded063dbe74c6d5dc8e1ec88bb29c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
