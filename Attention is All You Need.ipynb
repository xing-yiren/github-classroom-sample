{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概述\n",
    "\n",
    "与带有Attention机制的Seq2Seq所不同的是，Transformer模型是一个纯基于自注意力机制（self-attention mechanism）的架构，不包含任何循环神经网络（Recurrent Neural Network， RNN）和卷积神经网络（Convolutional Neural Network，CNN）。整个模型由\n",
    "## 数据准备\n",
    "\n",
    "我们本次使用的数据集为**Multi30K数据集**，它是一个大规模的图像-文本数据集，包含30K+图片，每张图片对应两类不同的文本描述：\n",
    "- 英语描述，及对应的德语翻译；\n",
    "- 五个独立的、非翻译而来的英语和德语描述，描述中包含的细节并不相同；\n",
    "\n",
    "因其收集的不同语言对于图片的描述相互独立，所以训练出的模型可以更好地适用于有噪声的多模态内容。\n",
    "\n",
    "![avatar](./assets/Multi30K.png)\n",
    "> 图片来源：Elliott, D., Frank, S., Sima’an, K., & Specia, L. (2016). Multi30K: Multilingual English-German Image Descriptions. CoRR, 1605.00459.\n",
    "\n",
    "首先，我们需要下载如下依赖：\n",
    "\n",
    "- 分词工具：`pip install spacy`\n",
    "- 德语/英语分词器：`python -m spacy download de_core_news_sm`，`python -m spacy download en_core_web_sm`\n",
    "\n",
    "### 数据下载模块\n",
    "\n",
    "使用`download`进行数据下载，并将`tar.gz`文件解压到指定文件夹。\n",
    "\n",
    "下载好的数据集目录结构如下：\n",
    "\n",
    "```text\n",
    "home_path/.mindspore_examples\n",
    "├─test\n",
    "│      test2016.de\n",
    "│      test2016.en\n",
    "│      test2016.fr\n",
    "│\n",
    "├─train\n",
    "│      train.de\n",
    "│      train.en\n",
    "│\n",
    "└─valid\n",
    "        val.de\n",
    "        val.en\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replace is False and data exists, so doing nothing. Use replace=True to re-download the data.\n",
      "Replace is False and data exists, so doing nothing. Use replace=True to re-download the data.\n",
      "Replace is False and data exists, so doing nothing. Use replace=True to re-download the data.\n"
     ]
    }
   ],
   "source": [
    "from download import download\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# 训练、验证、测试数据集下载地址\n",
    "urls = {\n",
    "    'train': 'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/training.tar.gz',\n",
    "    'valid': 'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz',\n",
    "    'test': 'http://www.quest.dcs.shef.ac.uk/wmt17_files_mmt/mmt_task1_test2016.tar.gz'\n",
    "}\n",
    "\n",
    "# 指定保存路径为 `home_path/.mindspore_examples`\n",
    "cache_dir = Path.home() / '.mindspore_examples'\n",
    "\n",
    "train_path = download(urls['train'], os.path.join(cache_dir, 'train'), kind='tar.gz')\n",
    "valid_path = download(urls['valid'], os.path.join(cache_dir, 'valid'), kind='tar.gz')\n",
    "test_path = download(urls['test'], os.path.join(cache_dir, 'test'), kind='tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理\n",
    "\n",
    "在使用数据进行模型训练等操作时，我们需要对数据进行预处理，流程如下：\n",
    "\n",
    "1. 加载数据集，目前数据为句子形式的文本，需要进行分词，即将句子拆解为单独的词元（token，可以为字符或者单词）；\n",
    "    - 分词可以使用`spaCy`创建分词器（tokenizer）：`de_core_news_sm`，`en_core_web_sm`，需要手动下载；\n",
    "    - 分词后，去除多余的空格，统一大小写等；\n",
    "2. 将每个词元映射到从0开始的数字索引中（为节约存储空间，可过滤掉词频低的词元），词元和数字索引所构成的集合叫做词典（vocabulary）；\n",
    "3. 添加特殊占位符，标明序列的起始与结束，统一序列长度，并创建数据迭代器；\n",
    "\n",
    "#### 数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from functools import partial\n",
    "\n",
    "class Multi30K():\n",
    "    \"\"\"Multi30K数据集加载器\n",
    "\n",
    "    加载Multi30K数据集并处理为一个Python迭代对象。\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.data = self._load(path)\n",
    "\n",
    "    def _load(self, path):\n",
    "        def tokenize(text, spacy_lang):\n",
    "            # 去除多余空格，统一大小写\n",
    "            text = text.rstrip()\n",
    "            return [tok.text.lower() for tok in spacy_lang.tokenizer(text)]\n",
    "\n",
    "        # 加载英、德语分词器\n",
    "        tokenize_de = partial(tokenize, spacy_lang=spacy.load('de_core_news_sm'))\n",
    "        tokenize_en = partial(tokenize, spacy_lang=spacy.load('en_core_web_sm'))\n",
    "\n",
    "        # 读取Multi30K数据，并进行分词\n",
    "        members = {i.split('.')[-1]: i for i in os.listdir(path)}\n",
    "        de_path = os.path.join(path, members['de'])\n",
    "        en_path = os.path.join(path, members['en'])\n",
    "        with open(de_path, 'r') as de_file:\n",
    "            de = de_file.readlines()[:-1]\n",
    "            de = [tokenize_de(i) for i in de]\n",
    "        with open(en_path, 'r') as en_file:\n",
    "            en = en_file.readlines()[:-1]\n",
    "            en = [tokenize_en(i) for i in en]\n",
    "\n",
    "        return list(zip(de, en))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, test_dataset = Multi30K(train_path), Multi30K(valid_path), Multi30K(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对解压和分词结果进行测试，打印测试数据集第一组英德语文本，可以看到每一个单词和标点符号已经被单独分离出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de = ['ein', 'mann', 'mit', 'einem', 'orangefarbenen', 'hut', ',', 'der', 'etwas', 'anstarrt', '.']\n",
      "en = ['a', 'man', 'in', 'an', 'orange', 'hat', 'starring', 'at', 'something', '.']\n"
     ]
    }
   ],
   "source": [
    "for de, en in test_dataset:\n",
    "    print(f'de = {de}')\n",
    "    print(f'en = {en}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"通过词频字典，构建词典\"\"\"\n",
    "\n",
    "    special_tokens = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "    def __init__(self, word_count_dict, min_freq=1):\n",
    "        self.word2idx = {}\n",
    "        for idx, tok in enumerate(self.special_tokens):\n",
    "            self.word2idx[tok] = idx\n",
    "\n",
    "        # 过滤低词频的词元\n",
    "        filted_dict = {\n",
    "            w: c\n",
    "            for w, c in word_count_dict.items() if c >= min_freq\n",
    "        }\n",
    "        for w, _ in filted_dict.items():\n",
    "            self.word2idx[w] = len(self.word2idx)\n",
    "\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "\n",
    "        self.bos_idx = self.word2idx['<bos>']  # 特殊占位符：序列开始\n",
    "        self.eos_idx = self.word2idx['<eos>']  # 特殊占位符：序列结束\n",
    "        self.pad_idx = self.word2idx['<pad>']  # 特殊占位符：补充字符\n",
    "        self.unk_idx = self.word2idx['<unk>']  # 特殊占位符：低词频词元或未曾出现的词元\n",
    "\n",
    "    def _word2idx(self, word):\n",
    "        \"\"\"单词映射至数字索引\"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            return self.unk_idx\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def _idx2word(self, idx):\n",
    "        \"\"\"数字索引映射至单词\"\"\"\n",
    "        if idx not in self.idx2word:\n",
    "            raise ValueError('input index is not in vocabulary.')\n",
    "        return self.idx2word[idx]\n",
    "\n",
    "    def encode(self, word_or_list):\n",
    "        \"\"\"将单个单词或单词数组映射至单个数字索引或数字索引数组\"\"\"\n",
    "        if isinstance(word_or_list, list):\n",
    "            return [self._word2idx(i) for i in word_or_list]\n",
    "        return self._word2idx(word_or_list)\n",
    "\n",
    "    def decode(self, idx_or_list):\n",
    "        \"\"\"将单个数字索引或数字索引数组映射至单个单词或单词数组\"\"\"\n",
    "        if isinstance(idx_or_list, list):\n",
    "            return [self._idx2word(i) for i in idx_or_list]\n",
    "        return self._idx2word(idx_or_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过自定义词频字典进行测试，我们可以看到词典已去除词频少于2的词元c，并加入了默认的四个特殊占位符，故词典整体长度为：4 - 1 + 4 = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count = {'a':20, 'b':10, 'c':1, 'd':2}\n",
    "\n",
    "vocab = Vocab(word_count, min_freq=2)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用`collections`中的`Counter`和`OrderedDict`统计英/德语每个单词在整体文本中出现的频率。构建词频字典，然后再将词频字典转为词典。\n",
    "\n",
    "在分配数字索引时有一个小技巧：常用的词元对应数值较小的索引，这样可以节约空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def build_vocab(dataset):\n",
    "    de_words, en_words = [], []\n",
    "    for de, en in dataset:\n",
    "        de_words.extend(de)\n",
    "        en_words.extend(en)\n",
    "\n",
    "    de_count_dict = OrderedDict(sorted(Counter(de_words).items(), key=lambda t: t[1], reverse=True))\n",
    "    en_count_dict = OrderedDict(sorted(Counter(en_words).items(), key=lambda t: t[1], reverse=True))\n",
    "\n",
    "    return Vocab(de_count_dict, min_freq=2), Vocab(en_count_dict, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in de vocabulary: 7853\n"
     ]
    }
   ],
   "source": [
    "de_vocab, en_vocab = build_vocab(train_dataset)\n",
    "print('Unique tokens in de vocabulary:', len(de_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据迭代器\n",
    "\n",
    "数据预处理的最后一步是创建数据迭代器，我们在进一步处理数据（包括批处理，添加起始和终止符号，统一序列长度）后，将数据以张量的形式返回。\n",
    "\n",
    "创建数据迭代器需要如下参数：\n",
    "\n",
    "- `dataset`：分词后的数据集\n",
    "- `de_vocab`：德语词典\n",
    "- `en_vocab`：英语词典\n",
    "- `batch_size`：批量大小，即一个batch中包含多少个序列\n",
    "- `max_len`：序列最大长度，为最长有效文本长度 + 2（序列开始、序列结束占位符），如不满则补齐，如超过则丢弃\n",
    "- `drop_remainder`：是否在最后一个batch未满时，丢弃该batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "\n",
    "class Iterator():\n",
    "    \"\"\"创建数据迭代器\"\"\"\n",
    "    def __init__(self, dataset, de_vocab, en_vocab, batch_size, max_len=32, drop_reminder=False):\n",
    "        self.dataset = dataset\n",
    "        self.de_vocab = de_vocab\n",
    "        self.en_vocab = en_vocab\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.max_len = max_len\n",
    "        self.drop_reminder = drop_reminder\n",
    "\n",
    "        length = len(self.dataset) // batch_size\n",
    "        self.len = length if drop_reminder else length + 1  # 批量数量\n",
    "\n",
    "    def __call__(self):\n",
    "        def pad(idx_list, vocab, max_len):\n",
    "            \"\"\"统一序列长度，并记录有效长度\"\"\"\n",
    "            idx_pad_list, idx_len = [], []\n",
    "            # 当前序列度超过最大长度时，将超出的部分丢弃；当前序列长度小于最大长度时，用占位符补齐\n",
    "            for i in idx_list:\n",
    "                if len(i) > max_len - 2:\n",
    "                    idx_pad_list.append(\n",
    "                        [vocab.bos_idx] + i[:max_len-2] + [vocab.eos_idx]\n",
    "                    )\n",
    "                    idx_len.append(max_len)\n",
    "                else:\n",
    "                    idx_pad_list.append(\n",
    "                        [vocab.bos_idx] + i + [vocab.eos_idx] + [vocab.pad_idx] * (max_len - len(i) - 2)\n",
    "                    )\n",
    "                    idx_len.append(len(i) + 2)\n",
    "            return idx_pad_list, idx_len\n",
    "\n",
    "        def sort_by_length(src, trg):\n",
    "            \"\"\"对德/英语的字段长度进行排序\"\"\"\n",
    "            data = zip(src, trg)\n",
    "            data = sorted(data, key=lambda t: len(t[0]), reverse=True)\n",
    "            return zip(*list(data))\n",
    "\n",
    "        def encode_and_pad(batch_data, max_len):\n",
    "            \"\"\"将批量中的文本数据转换为数字索引，并统一每个序列的长度\"\"\"\n",
    "            # 将当前批量数据中的词元转化为索引\n",
    "            src_data, trg_data = zip(*batch_data)\n",
    "            src_idx = [self.de_vocab.encode(i) for i in src_data]\n",
    "            trg_idx = [self.en_vocab.encode(i) for i in trg_data]\n",
    "\n",
    "            # 统一序列长度\n",
    "            src_idx, trg_idx = sort_by_length(src_idx, trg_idx)\n",
    "            src_idx_pad, src_len = pad(src_idx, de_vocab, max_len)\n",
    "            trg_idx_pad, _ = pad(trg_idx, en_vocab, max_len)\n",
    "\n",
    "            return src_idx_pad, src_len, trg_idx_pad\n",
    "\n",
    "        for i in range(self.len):\n",
    "            # 获取当前批量的数据\n",
    "            if i == self.len - 1 and not self.drop_reminder:\n",
    "                batch_data = self.dataset[i * self.batch_size:]\n",
    "            else:\n",
    "                batch_data = self.dataset[i * self.batch_size: (i+1) * self.batch_size]\n",
    "\n",
    "            src_idx, src_len, trg_idx = encode_and_pad(batch_data, self.max_len)\n",
    "            # 将序列数据转换为tensor\n",
    "            yield mindspore.Tensor(src_idx, mindspore.int32), \\\n",
    "                mindspore.Tensor(src_len, mindspore.int32), \\\n",
    "                mindspore.Tensor(trg_idx, mindspore.int32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = Iterator(train_dataset, de_vocab, en_vocab, batch_size=128, max_len=32, drop_reminder=True)\n",
    "valid_iterator = Iterator(valid_dataset, de_vocab, en_vocab, batch_size=128, max_len=32, drop_reminder=False)\n",
    "test_iterator = Iterator(test_dataset, de_vocab, en_vocab, batch_size=128, max_len=32, drop_reminder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型解析\n",
    "\n",
    "### Transformer 架构\n",
    "\n",
    "与Seq2Seq类似，编码器+解码器+中间信息传递\n",
    "\n",
    "### 位置编码（Positional Encoding）\n",
    "\n",
    "### 多头注意力（Multi-Head Attention）\n",
    "\n",
    "- 自注意力\n",
    "- 多头注意力\n",
    "- 带掩码的多头注意力\n",
    "\n",
    "### Add & Norm\n",
    "\n",
    "- Add\n",
    "- Norm： Layer Norm\n",
    "\n",
    "### 前馈神经网络（Feed-Forward Nerual Network，FFN）\n",
    "\n",
    "- Pointwise FFN\n",
    "\n",
    "### 编码器（Encoder）\n",
    "\n",
    "### 解码器（Decoder）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "\n",
    "d_model = 512  # Embedding层维度\n",
    "n_head = 8  # 多头感知机中头的数量\n",
    "n_layer = 6  # 编码器和解码器的层数\n",
    "d_ff = 2048  # 前馈神经网络维度\n",
    "max_len = 32  # 序列最大长度\n",
    "\n",
    "\n",
    "model = nn.Transformer()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = nn.Adam(model.trainable_params(), learning_rate=0.0001)\n",
    "\n",
    "def forward_fn(src, trg):\n",
    "    output = model(src, trg)\n",
    "    loss = loss_fn(output, trg)\n",
    "\n",
    "    return loss\n",
    "\n",
    "grad_fn = mindspore.value_and_grad(forward_fn, None, opt.parameters)\n",
    "\n",
    "def train_step(src, trg):\n",
    "    loss, grads = grad_fn(src, trg)\n",
    "    opt(grads)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from mindspore import Tensor\n",
    "import numpy as np\n",
    "\n",
    "def get_sinusoid_encoding_table(n_position, d_model):\n",
    "    def cal_angle(position, hid_idx):\n",
    "        return position / np.power(10000, 2 * (hid_idx // 2) / d_model)\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "    return Tensor(sinusoid_table, mindspore.float32)\n",
    "\n",
    "def embedding(input, vocab, max_len, d_model):\n",
    "    context_emb = nn.Embedding(len(vocab), d_model)\n",
    "    sinusoid_table = get_sinusoid_encoding_table(max_len, d_model)\n",
    "    pos_emb = nn.Embedding(sinusoid_table.shape[0], sinusoid_table.shape[1], embedding_table=sinusoid_table)\n",
    "    output = context_emb(input) + pos_emb(input)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterator, epoch=0):\n",
    "    \"\"\"模型训练\"\"\"\n",
    "    model.set_train(True)\n",
    "    num_batches = len(iterator)\n",
    "    total_loss = 0 # 所有batch训练loss的累加\n",
    "    total_steps = 0 # 训练步数\n",
    "\n",
    "    with tqdm(total=num_batches) as t:\n",
    "        t.set_description(f'Epoch: {epoch}')\n",
    "        for src, src_len, trg in iterator():\n",
    "            src_emb = embedding(src, de_vocab, max_len, d_model)\n",
    "            trg_emb = embedding(trg, en_vocab, max_len, d_model)\n",
    "            # print(src_emb.shape)\n",
    "            # print(trg_emb.shape)\n",
    "            loss = train_step(src_emb, trg_emb) # 当前batch的loss\n",
    "            total_loss += loss.asnumpy()\n",
    "            total_steps += 1\n",
    "            curr_loss = total_loss / total_steps # 当前的平均loss\n",
    "            t.set_postfix({'loss': f'{curr_loss:.2f}'})\n",
    "            t.update(1)\n",
    "    \n",
    "    return total_loss / total_steps\n",
    "\n",
    "\n",
    "def evaluate(iterator):\n",
    "    \"\"\"模型验证\"\"\"\n",
    "    model.set_train(False)\n",
    "    num_batches = len(iterator)\n",
    "    total_loss = 0 # 所有batch训练loss的累加\n",
    "    total_steps = 0 # 训练步数\n",
    "    \n",
    "    with tqdm(total=num_batches) as t:\n",
    "        for src, src_len, trg in iterator():\n",
    "            src_emb = embedding(src, de_vocab, max_len, d_model)\n",
    "            trg_emb = embedding(trg, en_vocab, max_len, d_model)\n",
    "            loss = forward_fn(src_emb, trg_emb) # 当前batch的loss\n",
    "            total_loss += loss.asnumpy()\n",
    "            total_steps += 1\n",
    "            curr_loss = total_loss / total_steps # 当前的平均loss\n",
    "            t.set_postfix({'loss': f'{curr_loss:.2f}'})\n",
    "            t.update(1)\n",
    "    \n",
    "    return total_loss / total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0:   0%|          | 0/226 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "For Primitive[Dtype], the input argument[input_x] must be a Tensor, CSRTensor or COOTensor, but got <class 'tuple'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [13], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m# mindspore.set_context(mode=mindspore.GRAPH_MODE)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m     12\u001b[0m     \u001b[39m# 模型训练，网络权重更新\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     train_loss \u001b[39m=\u001b[39m train(train_iterator, i)\n\u001b[0;32m     14\u001b[0m     \u001b[39m# 网络权重更新后对模型进行验证\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     valid_loss \u001b[39m=\u001b[39m evaluate(valid_iterator)\n",
      "Cell \u001b[1;32mIn [12], line 37\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(iterator, epoch)\u001b[0m\n\u001b[0;32m     34\u001b[0m trg_emb \u001b[39m=\u001b[39m embedding(trg, en_vocab, max_len, d_model)\n\u001b[0;32m     35\u001b[0m \u001b[39m# print(src_emb.shape)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39m# print(trg_emb.shape)\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m loss \u001b[39m=\u001b[39m train_step(src_emb, trg_emb) \u001b[39m# 当前batch的loss\u001b[39;00m\n\u001b[0;32m     38\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39masnumpy()\n\u001b[0;32m     39\u001b[0m total_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[1;32mIn [11], line 24\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(src, trg)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_step\u001b[39m(src, trg):\n\u001b[1;32m---> 24\u001b[0m     loss, grads \u001b[39m=\u001b[39m grad_fn(src, trg)\n\u001b[0;32m     25\u001b[0m     opt(grads)\n\u001b[0;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\composite\\base.py:605\u001b[0m, in \u001b[0;36m_Grad.__call__.<locals>.after_grad\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mafter_grad\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 605\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_(fn_, weights)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\common\\api.py:101\u001b[0m, in \u001b[0;36m_wrap_func.<locals>.wrapper\u001b[1;34m(*arg, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39m@wraps\u001b[39m(fn)\n\u001b[0;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39marg, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 101\u001b[0m     results \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49marg, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    102\u001b[0m     \u001b[39mreturn\u001b[39;00m _convert_python_data(results)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\composite\\base.py:582\u001b[0m, in \u001b[0;36m_Grad.__call__.<locals>.after_grad\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[39m@_wrap_func\u001b[39m\n\u001b[0;32m    581\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mafter_grad\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 582\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pynative_forward_run(fn, grad_, args, kwargs)\n\u001b[0;32m    583\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mgrad(fn, grad_, weights, grad_position, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    584\u001b[0m     out \u001b[39m=\u001b[39m _pynative_executor()\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\composite\\base.py:631\u001b[0m, in \u001b[0;36m_Grad._pynative_forward_run\u001b[1;34m(self, fn, grad, args, kwargs)\u001b[0m\n\u001b[0;32m    629\u001b[0m _pynative_executor\u001b[39m.\u001b[39mset_grad_flag(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    630\u001b[0m _pynative_executor\u001b[39m.\u001b[39mnew_graph(fn, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnew_kwargs)\n\u001b[1;32m--> 631\u001b[0m outputs \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_kwargs)\n\u001b[0;32m    632\u001b[0m _pynative_executor\u001b[39m.\u001b[39mend_graph(fn, outputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnew_kwargs)\n\u001b[0;32m    633\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "Cell \u001b[1;32mIn [11], line 16\u001b[0m, in \u001b[0;36mforward_fn\u001b[1;34m(src, trg)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_fn\u001b[39m(src, trg):\n\u001b[1;32m---> 16\u001b[0m     output \u001b[39m=\u001b[39m model(src, trg)\n\u001b[0;32m     17\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(output, trg)\n\u001b[0;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:657\u001b[0m, in \u001b[0;36mCell.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    656\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mclear_res()\n\u001b[1;32m--> 657\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m    659\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, Parameter):\n\u001b[0;32m    660\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mdata\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:653\u001b[0m, in \u001b[0;36mCell.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    652\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mnew_graph(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 653\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_construct(args, kwargs)\n\u001b[0;32m    654\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mend_graph(\u001b[39mself\u001b[39m, output, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:441\u001b[0m, in \u001b[0;36mCell._run_construct\u001b[1;34m(self, cast_inputs, kwargs)\u001b[0m\n\u001b[0;32m    439\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shard_fn(\u001b[39m*\u001b[39mcast_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstruct(\u001b[39m*\u001b[39;49mcast_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    442\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_forward_hook:\n\u001b[0;32m    443\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_forward_hook(cast_inputs, output)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\layer\\transformer.py:628\u001b[0m, in \u001b[0;36mTransformer.construct\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m src\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model \u001b[39mor\u001b[39;00m tgt\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model:\n\u001b[0;32m    626\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 628\u001b[0m memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(src, mask\u001b[39m=\u001b[39;49msrc_mask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask)\n\u001b[0;32m    629\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[39m=\u001b[39mtgt_mask, memory_mask\u001b[39m=\u001b[39mmemory_mask,\n\u001b[0;32m    630\u001b[0m                       tgt_key_padding_mask\u001b[39m=\u001b[39mtgt_key_padding_mask,\n\u001b[0;32m    631\u001b[0m                       memory_key_padding_mask\u001b[39m=\u001b[39mmemory_key_padding_mask)\n\u001b[0;32m    632\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:657\u001b[0m, in \u001b[0;36mCell.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    656\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mclear_res()\n\u001b[1;32m--> 657\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m    659\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, Parameter):\n\u001b[0;32m    660\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mdata\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:653\u001b[0m, in \u001b[0;36mCell.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    652\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mnew_graph(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 653\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_construct(args, kwargs)\n\u001b[0;32m    654\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mend_graph(\u001b[39mself\u001b[39m, output, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:441\u001b[0m, in \u001b[0;36mCell._run_construct\u001b[1;34m(self, cast_inputs, kwargs)\u001b[0m\n\u001b[0;32m    439\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shard_fn(\u001b[39m*\u001b[39mcast_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstruct(\u001b[39m*\u001b[39;49mcast_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    442\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_forward_hook:\n\u001b[0;32m    443\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_forward_hook(cast_inputs, output)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\layer\\transformer.py:479\u001b[0m, in \u001b[0;36mTransformerEncoder.construct\u001b[1;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    477\u001b[0m src_key_padding_mask_for_layers \u001b[39m=\u001b[39m src_key_padding_mask\n\u001b[0;32m    478\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> 479\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[0;32m    481\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    482\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(output)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:657\u001b[0m, in \u001b[0;36mCell.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    656\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mclear_res()\n\u001b[1;32m--> 657\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m    659\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, Parameter):\n\u001b[0;32m    660\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mdata\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:653\u001b[0m, in \u001b[0;36mCell.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    652\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mnew_graph(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 653\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_construct(args, kwargs)\n\u001b[0;32m    654\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mend_graph(\u001b[39mself\u001b[39m, output, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:441\u001b[0m, in \u001b[0;36mCell._run_construct\u001b[1;34m(self, cast_inputs, kwargs)\u001b[0m\n\u001b[0;32m    439\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shard_fn(\u001b[39m*\u001b[39mcast_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstruct(\u001b[39m*\u001b[39;49mcast_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    442\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_forward_hook:\n\u001b[0;32m    443\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_forward_hook(cast_inputs, output)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\layer\\transformer.py:312\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.construct\u001b[1;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    310\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[0;32m    311\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 312\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[0;32m    313\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[0;32m    315\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\layer\\transformer.py:318\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x, attn_mask, key_padding_mask):\n\u001b[1;32m--> 318\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[0;32m    319\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    320\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m    321\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    322\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\layer\\transformer.py:177\u001b[0m, in \u001b[0;36mMultiheadAttention.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_is_v \u001b[39m=\u001b[39m key \u001b[39mis\u001b[39;00m value\n\u001b[0;32m    176\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_is_k \u001b[39m=\u001b[39m query \u001b[39mis\u001b[39;00m key\n\u001b[1;32m--> 177\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:657\u001b[0m, in \u001b[0;36mCell.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    656\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mclear_res()\n\u001b[1;32m--> 657\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m    659\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, Parameter):\n\u001b[0;32m    660\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mdata\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:653\u001b[0m, in \u001b[0;36mCell.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    652\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mnew_graph(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 653\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_construct(args, kwargs)\n\u001b[0;32m    654\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mend_graph(\u001b[39mself\u001b[39m, output, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:441\u001b[0m, in \u001b[0;36mCell._run_construct\u001b[1;34m(self, cast_inputs, kwargs)\u001b[0m\n\u001b[0;32m    439\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shard_fn(\u001b[39m*\u001b[39mcast_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstruct(\u001b[39m*\u001b[39;49mcast_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    442\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_forward_hook:\n\u001b[0;32m    443\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_forward_hook(cast_inputs, output)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\layer\\transformer.py:212\u001b[0m, in \u001b[0;36mMultiheadAttention.construct\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[0;32m    200\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m multi_head_attention_forward(\n\u001b[0;32m    201\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[0;32m    202\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight, average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[0;32m    210\u001b[0m         k_is_v\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_is_v, q_is_k\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_is_k)\n\u001b[0;32m    211\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 212\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m multi_head_attention_forward(\n\u001b[0;32m    213\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[0;32m    214\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[0;32m    215\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[0;32m    216\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    217\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[0;32m    218\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m    219\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask, average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[0;32m    220\u001b[0m         k_is_v\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mk_is_v, q_is_k\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_is_k)\n\u001b[0;32m    222\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[0;32m    223\u001b[0m     attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mswapaxes(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\function\\nn_func.py:6043\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal, k_is_v, q_is_k)\u001b[0m\n\u001b[0;32m   6040\u001b[0m k \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m   6041\u001b[0m v \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[1;32m-> 6043\u001b[0m attn_output, attn_output_weights \u001b[39m=\u001b[39m _scaled_dot_product_attention(\n\u001b[0;32m   6044\u001b[0m     q, k, v, attn_mask, dropout_p, is_causal, training)\n\u001b[0;32m   6045\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mview(bsz \u001b[39m*\u001b[39m tgt_len, embed_dim)\n\u001b[0;32m   6047\u001b[0m attn_output \u001b[39m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\function\\nn_func.py:5842\u001b[0m, in \u001b[0;36m_scaled_dot_product_attention\u001b[1;34m(query, key, value, attn_mask, dropout_p, is_causal, is_training)\u001b[0m\n\u001b[0;32m   5840\u001b[0m \u001b[39mif\u001b[39;00m dropout_p \u001b[39m>\u001b[39m \u001b[39m0.\u001b[39m \u001b[39mand\u001b[39;00m is_training:\n\u001b[0;32m   5841\u001b[0m     attn \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mdropout(attn, dropout_p)\n\u001b[1;32m-> 5842\u001b[0m output \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mmatmul(attn, value)\n\u001b[0;32m   5844\u001b[0m \u001b[39mreturn\u001b[39;00m (output, attn)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\function\\math_func.py:7879\u001b[0m, in \u001b[0;36mmatmul\u001b[1;34m(x1, x2)\u001b[0m\n\u001b[0;32m   7876\u001b[0m shape_op \u001b[39m=\u001b[39m _get_cache_prim(P\u001b[39m.\u001b[39mShape)()\n\u001b[0;32m   7877\u001b[0m reshape_op \u001b[39m=\u001b[39m _get_cache_prim(P\u001b[39m.\u001b[39mReshape)()\n\u001b[1;32m-> 7879\u001b[0m dtype1 \u001b[39m=\u001b[39m dtype_op(x1)\n\u001b[0;32m   7880\u001b[0m dtype2 \u001b[39m=\u001b[39m dtype_op(x2)\n\u001b[0;32m   7881\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _check_same_type(dtype1, dtype2):\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\operations\\array_ops.py:263\u001b[0m, in \u001b[0;36mDType.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    262\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x, (Tensor, CSRTensor, COOTensor, Tensor_, CSRTensor_, COOTensor_)):\n\u001b[1;32m--> 263\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mFor Primitive[Dtype], the input argument[input_x] \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    264\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mmust be a Tensor, CSRTensor or COOTensor, but got \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mtype\u001b[39m(x)) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    265\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39mdtype\n",
      "\u001b[1;31mTypeError\u001b[0m: For Primitive[Dtype], the input argument[input_x] must be a Tensor, CSRTensor or COOTensor, but got <class 'tuple'>."
     ]
    }
   ],
   "source": [
    "from mindspore import save_checkpoint\n",
    "\n",
    "num_epochs = 10 # 训练迭代数\n",
    "clip = 1.0 # 梯度裁剪阈值\n",
    "best_valid_loss = float('inf') # 当前最佳验证损失\n",
    "ckpt_file_name = os.path.join(cache_dir, 'seq2seq.ckpt') # 模型保存路径\n",
    "\n",
    "mindspore.set_context(mode=mindspore.PYNATIVE_MODE)\n",
    "# mindspore.set_context(mode=mindspore.GRAPH_MODE)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    # 模型训练，网络权重更新\n",
    "    train_loss = train(train_iterator, i)\n",
    "    # 网络权重更新后对模型进行验证\n",
    "    valid_loss = evaluate(valid_iterator)\n",
    "    \n",
    "    # 保存当前效果最好的模型\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        save_checkpoint(model, ckpt_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "For Primitive[Dtype], the input argument[input_x] must be a Tensor, CSRTensor or COOTensor, but got <class 'tuple'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     optimizer(grads)\n\u001b[0;32m     18\u001b[0m net_work\u001b[39m.\u001b[39mset_train(\u001b[39mTrue\u001b[39;00m)   \n\u001b[1;32m---> 19\u001b[0m train_step(src, trg)\n",
      "Cell \u001b[1;32mIn [17], line 15\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(src, trg)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_step\u001b[39m(src, trg):\n\u001b[1;32m---> 15\u001b[0m     loss, grads \u001b[39m=\u001b[39m grad_fn(src, trg)\n\u001b[0;32m     16\u001b[0m     optimizer(grads)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\composite\\base.py:605\u001b[0m, in \u001b[0;36m_Grad.__call__.<locals>.after_grad\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mafter_grad\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 605\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_(fn_, weights)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\common\\api.py:101\u001b[0m, in \u001b[0;36m_wrap_func.<locals>.wrapper\u001b[1;34m(*arg, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39m@wraps\u001b[39m(fn)\n\u001b[0;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39marg, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 101\u001b[0m     results \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49marg, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    102\u001b[0m     \u001b[39mreturn\u001b[39;00m _convert_python_data(results)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\composite\\base.py:582\u001b[0m, in \u001b[0;36m_Grad.__call__.<locals>.after_grad\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[39m@_wrap_func\u001b[39m\n\u001b[0;32m    581\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mafter_grad\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 582\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pynative_forward_run(fn, grad_, args, kwargs)\n\u001b[0;32m    583\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mgrad(fn, grad_, weights, grad_position, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    584\u001b[0m     out \u001b[39m=\u001b[39m _pynative_executor()\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\composite\\base.py:631\u001b[0m, in \u001b[0;36m_Grad._pynative_forward_run\u001b[1;34m(self, fn, grad, args, kwargs)\u001b[0m\n\u001b[0;32m    629\u001b[0m _pynative_executor\u001b[39m.\u001b[39mset_grad_flag(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    630\u001b[0m _pynative_executor\u001b[39m.\u001b[39mnew_graph(fn, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnew_kwargs)\n\u001b[1;32m--> 631\u001b[0m outputs \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_kwargs)\n\u001b[0;32m    632\u001b[0m _pynative_executor\u001b[39m.\u001b[39mend_graph(fn, outputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnew_kwargs)\n\u001b[0;32m    633\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "Cell \u001b[1;32mIn [17], line 7\u001b[0m, in \u001b[0;36mforward_fn\u001b[1;34m(src, trg)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_fn\u001b[39m(src, trg):\n\u001b[1;32m----> 7\u001b[0m     out \u001b[39m=\u001b[39m net_work(src, trg)\n\u001b[0;32m      8\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(out, trg)\n\u001b[0;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:657\u001b[0m, in \u001b[0;36mCell.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    656\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mclear_res()\n\u001b[1;32m--> 657\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m    659\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, Parameter):\n\u001b[0;32m    660\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mdata\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:653\u001b[0m, in \u001b[0;36mCell.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    652\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mnew_graph(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 653\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_construct(args, kwargs)\n\u001b[0;32m    654\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mend_graph(\u001b[39mself\u001b[39m, output, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:441\u001b[0m, in \u001b[0;36mCell._run_construct\u001b[1;34m(self, cast_inputs, kwargs)\u001b[0m\n\u001b[0;32m    439\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shard_fn(\u001b[39m*\u001b[39mcast_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstruct(\u001b[39m*\u001b[39;49mcast_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    442\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_forward_hook:\n\u001b[0;32m    443\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_forward_hook(cast_inputs, output)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\layer\\transformer.py:628\u001b[0m, in \u001b[0;36mTransformer.construct\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m src\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model \u001b[39mor\u001b[39;00m tgt\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model:\n\u001b[0;32m    626\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 628\u001b[0m memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(src, mask\u001b[39m=\u001b[39;49msrc_mask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask)\n\u001b[0;32m    629\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[39m=\u001b[39mtgt_mask, memory_mask\u001b[39m=\u001b[39mmemory_mask,\n\u001b[0;32m    630\u001b[0m                       tgt_key_padding_mask\u001b[39m=\u001b[39mtgt_key_padding_mask,\n\u001b[0;32m    631\u001b[0m                       memory_key_padding_mask\u001b[39m=\u001b[39mmemory_key_padding_mask)\n\u001b[0;32m    632\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:657\u001b[0m, in \u001b[0;36mCell.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    656\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mclear_res()\n\u001b[1;32m--> 657\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m    659\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, Parameter):\n\u001b[0;32m    660\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mdata\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:653\u001b[0m, in \u001b[0;36mCell.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    652\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mnew_graph(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 653\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_construct(args, kwargs)\n\u001b[0;32m    654\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mend_graph(\u001b[39mself\u001b[39m, output, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:441\u001b[0m, in \u001b[0;36mCell._run_construct\u001b[1;34m(self, cast_inputs, kwargs)\u001b[0m\n\u001b[0;32m    439\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shard_fn(\u001b[39m*\u001b[39mcast_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstruct(\u001b[39m*\u001b[39;49mcast_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    442\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_forward_hook:\n\u001b[0;32m    443\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_forward_hook(cast_inputs, output)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\layer\\transformer.py:479\u001b[0m, in \u001b[0;36mTransformerEncoder.construct\u001b[1;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    477\u001b[0m src_key_padding_mask_for_layers \u001b[39m=\u001b[39m src_key_padding_mask\n\u001b[0;32m    478\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> 479\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[0;32m    481\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    482\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(output)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:657\u001b[0m, in \u001b[0;36mCell.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    656\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mclear_res()\n\u001b[1;32m--> 657\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m    659\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, Parameter):\n\u001b[0;32m    660\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mdata\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:653\u001b[0m, in \u001b[0;36mCell.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    652\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mnew_graph(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 653\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_construct(args, kwargs)\n\u001b[0;32m    654\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mend_graph(\u001b[39mself\u001b[39m, output, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:441\u001b[0m, in \u001b[0;36mCell._run_construct\u001b[1;34m(self, cast_inputs, kwargs)\u001b[0m\n\u001b[0;32m    439\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shard_fn(\u001b[39m*\u001b[39mcast_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstruct(\u001b[39m*\u001b[39;49mcast_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    442\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_forward_hook:\n\u001b[0;32m    443\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_forward_hook(cast_inputs, output)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\layer\\transformer.py:312\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.construct\u001b[1;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    310\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[0;32m    311\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 312\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[0;32m    313\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[0;32m    315\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\layer\\transformer.py:318\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x, attn_mask, key_padding_mask):\n\u001b[1;32m--> 318\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[0;32m    319\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    320\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m    321\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    322\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\layer\\transformer.py:177\u001b[0m, in \u001b[0;36mMultiheadAttention.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_is_v \u001b[39m=\u001b[39m key \u001b[39mis\u001b[39;00m value\n\u001b[0;32m    176\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_is_k \u001b[39m=\u001b[39m query \u001b[39mis\u001b[39;00m key\n\u001b[1;32m--> 177\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:657\u001b[0m, in \u001b[0;36mCell.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    656\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mclear_res()\n\u001b[1;32m--> 657\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m    659\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, Parameter):\n\u001b[0;32m    660\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mdata\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:653\u001b[0m, in \u001b[0;36mCell.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    652\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mnew_graph(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 653\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_construct(args, kwargs)\n\u001b[0;32m    654\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mend_graph(\u001b[39mself\u001b[39m, output, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\cell.py:441\u001b[0m, in \u001b[0;36mCell._run_construct\u001b[1;34m(self, cast_inputs, kwargs)\u001b[0m\n\u001b[0;32m    439\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shard_fn(\u001b[39m*\u001b[39mcast_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstruct(\u001b[39m*\u001b[39;49mcast_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    442\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_forward_hook:\n\u001b[0;32m    443\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_forward_hook(cast_inputs, output)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\nn\\layer\\transformer.py:212\u001b[0m, in \u001b[0;36mMultiheadAttention.construct\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[0;32m    200\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m multi_head_attention_forward(\n\u001b[0;32m    201\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[0;32m    202\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight, average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[0;32m    210\u001b[0m         k_is_v\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_is_v, q_is_k\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_is_k)\n\u001b[0;32m    211\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 212\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m multi_head_attention_forward(\n\u001b[0;32m    213\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[0;32m    214\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[0;32m    215\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[0;32m    216\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    217\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[0;32m    218\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m    219\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask, average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[0;32m    220\u001b[0m         k_is_v\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mk_is_v, q_is_k\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_is_k)\n\u001b[0;32m    222\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[0;32m    223\u001b[0m     attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mswapaxes(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\function\\nn_func.py:6043\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal, k_is_v, q_is_k)\u001b[0m\n\u001b[0;32m   6040\u001b[0m k \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m   6041\u001b[0m v \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[1;32m-> 6043\u001b[0m attn_output, attn_output_weights \u001b[39m=\u001b[39m _scaled_dot_product_attention(\n\u001b[0;32m   6044\u001b[0m     q, k, v, attn_mask, dropout_p, is_causal, training)\n\u001b[0;32m   6045\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mview(bsz \u001b[39m*\u001b[39m tgt_len, embed_dim)\n\u001b[0;32m   6047\u001b[0m attn_output \u001b[39m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\function\\nn_func.py:5842\u001b[0m, in \u001b[0;36m_scaled_dot_product_attention\u001b[1;34m(query, key, value, attn_mask, dropout_p, is_causal, is_training)\u001b[0m\n\u001b[0;32m   5840\u001b[0m \u001b[39mif\u001b[39;00m dropout_p \u001b[39m>\u001b[39m \u001b[39m0.\u001b[39m \u001b[39mand\u001b[39;00m is_training:\n\u001b[0;32m   5841\u001b[0m     attn \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mdropout(attn, dropout_p)\n\u001b[1;32m-> 5842\u001b[0m output \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mmatmul(attn, value)\n\u001b[0;32m   5844\u001b[0m \u001b[39mreturn\u001b[39;00m (output, attn)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\function\\math_func.py:7879\u001b[0m, in \u001b[0;36mmatmul\u001b[1;34m(x1, x2)\u001b[0m\n\u001b[0;32m   7876\u001b[0m shape_op \u001b[39m=\u001b[39m _get_cache_prim(P\u001b[39m.\u001b[39mShape)()\n\u001b[0;32m   7877\u001b[0m reshape_op \u001b[39m=\u001b[39m _get_cache_prim(P\u001b[39m.\u001b[39mReshape)()\n\u001b[1;32m-> 7879\u001b[0m dtype1 \u001b[39m=\u001b[39m dtype_op(x1)\n\u001b[0;32m   7880\u001b[0m dtype2 \u001b[39m=\u001b[39m dtype_op(x2)\n\u001b[0;32m   7881\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _check_same_type(dtype1, dtype2):\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\mindspore_2.0_0227\\lib\\site-packages\\mindspore\\ops\\operations\\array_ops.py:263\u001b[0m, in \u001b[0;36mDType.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    262\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x, (Tensor, CSRTensor, COOTensor, Tensor_, CSRTensor_, COOTensor_)):\n\u001b[1;32m--> 263\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mFor Primitive[Dtype], the input argument[input_x] \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    264\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mmust be a Tensor, CSRTensor or COOTensor, but got \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mtype\u001b[39m(x)) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    265\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39mdtype\n",
      "\u001b[1;31mTypeError\u001b[0m: For Primitive[Dtype], the input argument[input_x] must be a Tensor, CSRTensor or COOTensor, but got <class 'tuple'>."
     ]
    }
   ],
   "source": [
    "net_work = nn.Transformer()\n",
    "\n",
    "src = Tensor(np.random.rand(128, 32, 512), mindspore.float32)\n",
    "trg = Tensor(np.random.rand(128, 32, 512), mindspore.float32)\n",
    "\n",
    "def forward_fn(src, trg):\n",
    "    out = net_work(src, trg)\n",
    "    loss = loss_fn(out, trg)\n",
    "    return loss\n",
    "\n",
    "optimizer = nn.Adam(net_work.trainable_params(), learning_rate=0.0001)\n",
    "grad_fn = mindspore.value_and_grad(forward_fn, None, optimizer.parameters)\n",
    "\n",
    "def train_step(src, trg):\n",
    "    loss, grads = grad_fn(src, trg)\n",
    "    optimizer(grads)\n",
    "\n",
    "net_work.set_train(True)   \n",
    "train_step(src, trg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_work = nn.Transformer()\n",
    "\n",
    "src = Tensor(np.random.rand(128, 32, 512), mindspore.float32)\n",
    "trg = Tensor(np.random.rand(128, 32, 512), mindspore.float32)\n",
    "\n",
    "def forward_fn(src, trg):\n",
    "    out = net_work(src, trg)\n",
    "    loss = loss_fn(out, trg)\n",
    "    return loss\n",
    "\n",
    "optimizer = nn.Adam(net_work.trainable_params(), learning_rate=0.0001)\n",
    "grad_fn = mindspore.value_and_grad(forward_fn, None, optimizer.parameters)\n",
    "\n",
    "def train_step(src, trg):\n",
    "    loss, grads = grad_fn(src, trg)\n",
    "    optimizer(grads)\n",
    "\n",
    "# net_work.set_train(True)   \n",
    "train_step(src, trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "05c517b50f41e6be7144c1cbb587776c76406ba434488b73d8ff27b6b4bbd40f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
